# プロジェクト概要

## プロジェクト名
cat-brain

## 概要
本プロジェクトは「ねこ育成ゲーム」を開発することを目的としています。最大の特徴は「強化学習で動くねこ」を実現し、より自然でリアルなねこの行動をゲーム内で再現することです。

## 対象ユーザー
- 動物が好きだが、実際に飼うことができない人
- ゲーム内でできるだけリアルなねこの飼育体験を求める人

## 技術構成
- cat-dqn: 強化学習（DQN/RAINBOW DQNベース）によるねこAIのソースコード
  - Pythonで実装。RNNやPrioritized Replay Buffer、Optical-Critic構造も導入。
- cat-game: ReactによるSPA（Single Page Application）で構築されたゲーム本体
  - JavaScript/JSX、Vite利用。ONNX Runtime WebでAIモデルをブラウザ推論。
  - S3等での静的ホスティングを前提とし、デプロイ・管理の簡易化を重視。
  - Python側と共通の設定（観測空間・行動空間等）はcommon.jsonで管理。

## cat-game（フロントエンド）設計意図
- スタート画面（StartScreen）：ゲーム説明・操作説明・スタートボタン。
- ゲーム画面（GameScreen）：ねこ（AI）とおもちゃ（プレイヤー操作）が登場。
- おもちゃ操作UI（GameControls）：アナログスティック、速度・モード切替等。
- プレイヤーはおもちゃを操作し、ねこはAIモデル（ONNX）による自律行動。
- 難易度や速度の切替も可能。
- AIモデルの入替や設定変更が容易。
- プレイヤーが「ねこと遊ぶ」体験を重視し、操作性・レスポンスを意識。
## 強化学習設計方針
- RAINBOW DQNを採用。十分な性能が出ているためこの構造を基本とする。
- ねこが環境（おもちゃ等）の動きを理解できるようRNNを導入。
- Prioritized Replay Bufferも導入し、効率的な学習を実現。
- Actor-Criticも検討したが、RAINBOWで十分なため現状は未採用。
- Optical-Critic構造も併用し、行動毎にDQNを学習できるメリットを活かしている。
- 設計方針は「学習に対して性能が出ること」を最優先。

## ねこAIの行動原理・目指す自然さ
- 現状実装している行動：
  - おもちゃを追いかける（動くものに反応して行動）
  - 止まる（興味がないときに休憩）
- 今後拡張予定の行動：
  - 寝る、毛づくろい、食べる、狩り、飼い主に懐く等
- 理想は「全ての猫の行動」を再現することだが、まずはゲームとして成立する行動から実装。

## ねこ性格・行動パラメータ設計方針
ねこの性格や行動傾向（例：好奇心、警戒心、活発さ等）は、まずは「環境・報酬設計」によって誘導する方針とします。

- 例：
  - 体力（エネルギー）消費量・基礎代謝・捕食による回復量
  - おもちゃやダミーの出現率
  - 体力の初期値・最大値、長期体力・短期体力の分離
  - 報酬設計や環境パラメータの調整による性格誘導

この方針により、強化学習の本質（環境適応・自律的な学習）を活かしつつ、モデルの汎用性・拡張性を高めます。

ただし、環境設計だけで十分に性格を分離・誘導できない場合は、DQN側のロジックやパラメータ化（例：性格パラメータを状態や報酬に組み込む）も柔軟に許容します。

今後は、common.json等のコンフィグでこれらのパラメータを管理し、ねこの性格や行動傾向を切り替えられる設計を目指します。

## 環境設計の意図
- 強化学習のみでねこを学習させるため、マルチエージェント環境（PettingZoo）を採用。
- おもちゃ等の教師データを用意せず、ねこと獲物が共に学習・進化する構成。
- PettingZooはGymnasiumに近いインターフェースで、マルチエージェント環境として一般的なため選定。

## 開発・運用
- まずは動くコードを作ることを重視（強化学習は実装＝性能向上ではないため、最小限の改修で性能担保→必要に応じてリファクタ）
- developブランチは開発途中の資材を自由にマージ可能
- mainブランチはリリース毎にマージし、性能が担保された確実に動くコードのみ格納
- GitHub Issueを活用したプロジェクト管理（命名ルールは既存履歴を参照）
- 短期間でのリリースを重視し、現在はv9リリースに向けて作業中
