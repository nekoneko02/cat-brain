{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkqesDnh1Kcg",
        "outputId": "7262d3e8-3634-4977-faa3-e2df5ec050a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 0s (6,473 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Collecting pettingzoo[all]\n",
            "  Using cached pettingzoo-1.25.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[all]) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[all]) (1.1.1)\n",
            "Collecting multi_agent_ale_py>=0.1.11 (from pettingzoo[all])\n",
            "  Using cached multi-agent-ale-py-0.1.11.tar.gz (551 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[all]) (2.6.1)\n",
            "Collecting chess>=1.9.4 (from pettingzoo[all])\n",
            "  Using cached chess-1.11.2-py3-none-any.whl\n",
            "Collecting rlcard>=1.0.5 (from pettingzoo[all])\n",
            "  Using cached rlcard-1.2.0.tar.gz (269 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting shimmy>=1.2.0 (from shimmy[openspiel]>=1.2.0; extra == \"all\"->pettingzoo[all])\n",
            "  Using cached Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pymunk>=6.2.0 (from pettingzoo[all])\n",
            "  Using cached pymunk-6.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.1 kB)\n",
            "Collecting box2d-py>=2.3.5 (from pettingzoo[all])\n",
            "  Using cached box2d-py-2.3.8.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[all]) (1.15.2)\n",
            "Requirement already satisfied: pillow>=8.0.1 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[all]) (11.2.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[all]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[all]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->pettingzoo[all]) (0.0.4)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from pymunk>=6.2.0->pettingzoo[all]) (1.17.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from rlcard>=1.0.5->pettingzoo[all]) (3.0.1)\n",
            "Collecting open-spiel>=1.2 (from shimmy[openspiel]>=1.2.0; extra == \"all\"->pettingzoo[all])\n",
            "  Using cached open_spiel-1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.17.1->pymunk>=6.2.0->pettingzoo[all]) (2.22)\n",
            "Requirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.11/dist-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"all\"->pettingzoo[all]) (24.1.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.11/dist-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"all\"->pettingzoo[all]) (25.3.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"all\"->pettingzoo[all]) (1.4.0)\n",
            "Collecting ml-collections>=0.1.1 (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"all\"->pettingzoo[all])\n",
            "  Using cached ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from ml-collections>=0.1.1->open-spiel>=1.2->shimmy[openspiel]>=1.2.0; extra == \"all\"->pettingzoo[all]) (6.0.2)\n",
            "Using cached pymunk-6.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Using cached Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Using cached pettingzoo-1.25.0-py3-none-any.whl (852 kB)\n",
            "Using cached open_spiel-1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "Using cached ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "Building wheels for collected packages: box2d-py, multi_agent_ale_py, rlcard\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp311-cp311-linux_x86_64.whl size=2351233 sha256=0c659d18b200a6346ca281b1ba0f18baa06449c374e405e57097d4f4629d2241\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/95/02/4cb5adc9f6dcaeb9639c2271f630a66ab4440102414804c45c\n",
            "  Building wheel for multi_agent_ale_py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for multi_agent_ale_py: filename=multi_agent_ale_py-0.1.11-cp311-cp311-linux_x86_64.whl size=721821 sha256=2853e725968e25db0d5385bfa23336edd9427a8dfea68408a5a2c6c0c4c08aad\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/81/76/771ec8e34292c8a71dd6c4a52a1c0401f4d93cbfb54e02fce4\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-1.2.0-py3-none-any.whl size=325795 sha256=8efd0bbcea74ab4fcc3311f8ff886802d94759fbd202067c55068a0bb66d1508\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/85/10/9ce42203776b06686ffe36d932983a79be6dfa95638ecf500b\n",
            "Successfully built box2d-py multi_agent_ale_py rlcard\n",
            "Installing collected packages: box2d-py, rlcard, multi_agent_ale_py, ml-collections, chess, shimmy, pymunk, pettingzoo, open-spiel\n",
            "Successfully installed box2d-py-2.3.8 chess-1.11.2 ml-collections-1.1.0 multi_agent_ale_py-0.1.11 open-spiel-1.5 pettingzoo-1.25.0 pymunk-6.11.1 rlcard-1.2.0 shimmy-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!apt install cmake swig zlib1g-dev\n",
        "%pip install torch torchvision\n",
        "%pip install numpy protobuf onnx\n",
        "%pip install pettingzoo[all]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1AXujtu1Kci"
      },
      "source": [
        "# 強化学習モデルの学習 (main.py)\n",
        "\n",
        "このセルでは、DQNアルゴリズムを用いて、`CartPole-v1`環境でモデルを学習させます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz-KT21E1Kcj"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.test import api_test\n",
        "from cat_toy_env import CatToyEnv\n",
        "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
        "\n",
        "# 1個だけ環境を作る（並列ではなく）\n",
        "env = CatToyEnv(**env_kwargs)\n",
        "api_test(env, num_cycles=1000, verbose_progress=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo import AECEnv\n",
        "from pettingzoo.utils.agent_selector import agent_selector\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import json\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import pygame\n",
        "\n",
        "class CatToyEnv(AECEnv):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"name\": \"cat_toy_env_v0\"}\n",
        "\n",
        "    def __init__(self, render_mode=None, max_steps=1000):\n",
        "        super().__init__()\n",
        "        self.render_mode = render_mode\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "        f = \"\"\"\n",
        "  {\n",
        "  \"description\": \"PythonとJavaScriptの両方で使用する設定ファイル\",\n",
        "  \"actions\":{\n",
        "    \"cat\": [\n",
        "      { \"id\": 0, \"name\": \"up\", \"dx\": 0, \"dy\": -3 },\n",
        "      { \"id\": 1, \"name\": \"down\", \"dx\": 0, \"dy\": 3 },\n",
        "      { \"id\": 2, \"name\": \"left\", \"dx\": -3, \"dy\": 0 },\n",
        "      { \"id\": 3, \"name\": \"right\", \"dx\": 3, \"dy\": 0 }\n",
        "    ],\n",
        "    \"toy\": [\n",
        "      { \"id\": 0, \"name\": \"up\", \"dx\": 0, \"dy\": -1 },\n",
        "      { \"id\": 1, \"name\": \"down\", \"dx\": 0, \"dy\": 1 },\n",
        "      { \"id\": 2, \"name\": \"left\", \"dx\": -1, \"dy\": 0 },\n",
        "      { \"id\": 3, \"name\": \"right\", \"dx\": 1, \"dy\": 0 }\n",
        "    ]\n",
        "  },\n",
        "  \"observation_space\": {\n",
        "    \"low\": 0,\n",
        "    \"high\": 800,\n",
        "    \"shape\": [4],\n",
        "    \"dtype\": \"float32\"\n",
        "  },\n",
        "  \"environment\": {\n",
        "    \"width\": 800,\n",
        "    \"height\": 600,\n",
        "    \"cat_width\": 30,\n",
        "    \"cat_height\": 30,\n",
        "    \"toy_width\": 20,\n",
        "    \"toy_height\": 20,\n",
        "    \"state_scale\": 0.1\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "        config = json(f)\n",
        "\n",
        "        env_config = config['environment']\n",
        "        self.width = env_config['width']\n",
        "        self.height = env_config['height']\n",
        "        self.cat_width = env_config['cat_width']\n",
        "        self.cat_height = env_config['cat_height']\n",
        "        self.toy_width = env_config['toy_width']\n",
        "        self.toy_height = env_config['toy_height']\n",
        "        self.state_scale = env_config['state_scale']\n",
        "\n",
        "        self.actions = config['actions']\n",
        "\n",
        "        self.possible_agents = ['cat', 'toy']\n",
        "        self.agents = self.possible_agents[:]\n",
        "        self._agent_selector = agent_selector(self.agents)\n",
        "        self.agent_selection = self._agent_selector.next()\n",
        "\n",
        "        self.observation_spaces = {\n",
        "            agent: spaces.Box(low=0, high=1, shape=(1, int(self.height*self.state_scale), int(self.width*self.state_scale)), dtype=np.float32)\n",
        "            for agent in self.possible_agents\n",
        "        }\n",
        "        self.action_spaces = {\n",
        "            \"cat\": spaces.Discrete(len(self.actions[\"cat\"])),\n",
        "            \"toy\": spaces.Discrete(len(self.actions[\"toy\"])),\n",
        "        }\n",
        "\n",
        "        # ✅ PettingZoo AECEnv に必要な属性\n",
        "        self._cumulative_rewards = {agent: 0.0 for agent in self.agents}\n",
        "        self.rewards = {agent: 0.0 for agent in self.agents}\n",
        "        self.terminations = {agent: False for agent in self.agents}\n",
        "        self.truncations = {agent: False for agent in self.agents}\n",
        "        self.infos = {agent: {} for agent in self.agents}\n",
        "        self.step_count = 0\n",
        "\n",
        "    def observe(self, agent):\n",
        "        # 1/10にスケールして、stateを返す\n",
        "        scaled_cat_x = int(self.cat_x * self.state_scale)\n",
        "        scaled_cat_y = int(self.cat_y * self.state_scale)\n",
        "        scaled_toy_x = int(self.toy_x * self.state_scale)\n",
        "        scaled_toy_y = int(self.toy_y * self.state_scale)\n",
        "\n",
        "        obs = np.zeros(self.observation_spaces[\"cat\"].shape, dtype=np.float32)\n",
        "\n",
        "        # Cat の位置（矩形領域）を 1 に設定\n",
        "        obs[0,\n",
        "            scaled_cat_y:scaled_cat_y + int(self.cat_height*self.state_scale),\n",
        "            scaled_cat_x:scaled_cat_x + int(self.cat_width*self.state_scale)\n",
        "        ] = 1.0\n",
        "        # Toy の位置（矩形領域）を 0.5 に設定（オーバーラップ判定のため）\n",
        "        obs[0,\n",
        "            scaled_toy_y:scaled_toy_y + int(self.toy_height*self.state_scale),\n",
        "            scaled_toy_x:scaled_toy_x + int(self.toy_width*self.state_scale)\n",
        "        ] = 0.5\n",
        "\n",
        "        return obs\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.agents = self.possible_agents[:]\n",
        "        self._agent_selector = agent_selector(self.agents)\n",
        "        self.agent_selection = self._agent_selector.next()\n",
        "\n",
        "        self.rewards = {a: 0.0 for a in self.agents}\n",
        "        self._cumulative_rewards = {a: 0.0 for a in self.agents}\n",
        "        self.terminations = {a: False for a in self.agents}\n",
        "        self.truncations = {a: False for a in self.agents}\n",
        "        self.dones = {a: False for a in self.agents}  # 互換性のために残してもOK\n",
        "        self.infos = {a: {} for a in self.agents}\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.cat_x = random.randint(0, self.width - self.cat_width)\n",
        "        self.cat_y = random.randint(0, self.height - self.cat_height)\n",
        "        self.toy_x = random.randint(0, self.width - self.toy_width)\n",
        "        self.toy_y = random.randint(0, self.height - self.toy_height)\n",
        "\n",
        "        return self.observe(self.agent_selection)\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        agent = self.agent_selection\n",
        "\n",
        "        if self.terminations[agent] or self.truncations[agent]:\n",
        "            self._was_dead_step(action)\n",
        "            return\n",
        "\n",
        "        selected_action = self.actions[agent][action]\n",
        "        dx = selected_action[\"dx\"]\n",
        "        dy = selected_action[\"dy\"]\n",
        "\n",
        "        if agent == \"cat\":\n",
        "            self.cat_x = np.clip(self.cat_x + dx, 0, self.width - self.cat_width)\n",
        "            self.cat_y = np.clip(self.cat_y + dy, 0, self.height - self.cat_height)\n",
        "        elif agent == \"toy\":\n",
        "            self.toy_x = np.clip(self.toy_x + dx, 0, self.width - self.toy_width)\n",
        "            self.toy_y = np.clip(self.toy_y + dy, 0, self.height - self.toy_height)\n",
        "\n",
        "        collision = self._is_collision()\n",
        "        if collision:\n",
        "            self.terminations = {a: True for a in self.agents}\n",
        "            self.rewards[\"cat\"] = 100.0\n",
        "            self.rewards[\"toy\"] = -100.0\n",
        "        elif self.step_count >= self.max_steps:\n",
        "            self.truncations = {a: True for a in self.agents}\n",
        "        else:\n",
        "            distance = ((self.cat_x - self.toy_x) ** 2 + (self.cat_y - self.toy_y) ** 2) ** 0.5\n",
        "            self.rewards[\"cat\"] = -distance\n",
        "            self.rewards[\"toy\"] = distance\n",
        "        if self.step_count >= self.max_steps:\n",
        "            self.truncations[agent] = True\n",
        "\n",
        "        # ✅ 報酬加算\n",
        "        self._cumulative_rewards[agent] += self.rewards[agent]\n",
        "        self.step_count += 1\n",
        "\n",
        "        # ✅ 次のエージェントへ切り替え\n",
        "        self.agent_selection = self._agent_selector.next()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "    def _is_collision(self):\n",
        "        return (\n",
        "            self.cat_x < self.toy_x + self.toy_width and\n",
        "            self.cat_x + self.cat_width > self.toy_x and\n",
        "            self.cat_y < self.toy_y + self.toy_height and\n",
        "            self.cat_y + self.cat_height > self.toy_y\n",
        "        )\n",
        "\n",
        "    def render(self):\n",
        "        grid_size = 30  # 小さなグリッドに変更\n",
        "        grid = [[\".\" for _ in range(grid_size)] for _ in range(grid_size)]\n",
        "\n",
        "        # CatとToyの位置に記号を配置（同じ場所なら C&T と表示）\n",
        "        if self.cat_x == self.toy_x and self.cat_y == self.toy_y:\n",
        "            grid[self.cat_y*grid_size//self.height][self.cat_x*grid_size//self.width] = \"C&T\"\n",
        "        else:\n",
        "            grid[self.cat_y*grid_size//self.height][self.cat_x*grid_size//self.width] = \"C\"\n",
        "            grid[self.toy_y*grid_size//self.height][self.toy_x*grid_size//self.width] = \"T\"\n",
        "\n",
        "        # (ipynbだけ)ターミナルをクリアするためにclear_outputを使用\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        # グリッドを出力（y=0が上になるように反転）\n",
        "        for row in reversed(grid):\n",
        "            print(\" \".join(row))  # 一行ごとに表示\n",
        "        print(\"-\" * (2 * grid_size))\n",
        "\n",
        "        # その他の情報を表示\n",
        "        print(f\"agent: {self.agent_selection}, count: {self.step_count}, cat: {self.cat_x}, {self.cat_y}, toy: {self.toy_x}, {self.toy_y}\")\n",
        "\n",
        "        # フレーム間の遅延（1フレームごとの更新時間）\n",
        "        time.sleep(0.05)  # 0.5秒ごとに更新（調整可能）\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        pass  # 特にリソース解放がなければ空でOK"
      ],
      "metadata": {
        "id": "DVL0n-oG4WAi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3FZFULGI1Kcj"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YmXaM6bd1Kck"
      },
      "outputs": [],
      "source": [
        "num_iterations = 10000\n",
        "num_episodes_per_iteration = 1\n",
        "num_steps_per_episode = 1000\n",
        "# num_epoches = 1\n",
        "# num_replays_per_episode = num_epoches * num_episodes_per_iteration * num_steps_per_episode\n",
        "update_target_steps = 10\n",
        "replay_interval = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "9Kecc2h61Kck",
        "outputId": "c420e465-2b61-48cf-f1ff-98474b20808a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-42466a4a5abe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_steps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 1個だけ環境を作る\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv_preview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatToyEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0menv_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_preview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ea47d76aa6e8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, render_mode, max_steps)\u001b[0m\n\u001b[1;32m     52\u001b[0m }\n\u001b[1;32m     53\u001b[0m \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0menv_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'environment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "env_kwargs=dict(render_mode=None, max_steps = num_steps_per_episode)\n",
        "# 1個だけ環境を作る\n",
        "env_preview = CatToyEnv(**env_kwargs)\n",
        "\n",
        "obs = env_preview.reset()\n",
        "\n",
        "# 観測のshapeを確認\n",
        "print(\"観測の形:\", obs)\n",
        "print(\"観測の中身:\", obs)\n",
        "# 学習用環境\n",
        "env_learning = CatToyEnv(**env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9UXyofpi1Kck"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# CNNを使ったQネットワーク\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        c, h, w = input_shape  # (1, H, W)\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(c, 2, kernel_size=3, stride=1, padding=1),  # (8, H, W)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),                                   # (8, H/2, W/2)\n",
        "\n",
        "            nn.Conv2d(2, 4, kernel_size=3, stride=1, padding=1), # (16, H/2, W/2)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),                                    # (16, H/4, W/4),                                   # (8, H/2, W/2)\n",
        "\n",
        "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # 畳み込み後の特徴量サイズを計算\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, *input_shape)\n",
        "            conv_out = self.conv(dummy_input)\n",
        "            conv_out_size = conv_out.view(1, -1).size(1)\n",
        "            print(\"Conv出力の形:\", conv_out.shape)\n",
        "\n",
        "        # 全結合層\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        return self.fc(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.agent_name = agent_name  # エージェント名（'cat' または 'toy'）\n",
        "        self.action_space = env.action_spaces[self.agent_name]  # 各エージェントに対応するアクション空間\n",
        "        self.state_shape = env.observation_spaces[self.agent_name].shape  # 例: (1, H, W)\n",
        "        self.model = DQN(self.state_shape, self.action_space.n)\n",
        "        self.target_model = DQN(self.state_shape, self.action_space.n)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.batch_size = 64\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((\n",
        "            np.array(state, dtype=np.float32),\n",
        "            action,\n",
        "            reward,\n",
        "            np.array(next_state, dtype=np.float32),\n",
        "            done\n",
        "        ))\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return self.action_space.sample()  # ランダム行動\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)  # バッチ次元を追加\n",
        "        q_values = self.model(state)\n",
        "        return torch.argmax(q_values).item()  # 最大Q値に基づいて行動を選択\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(np.array(states))\n",
        "        next_states = torch.FloatTensor(np.array(next_states))\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        # Q値の計算\n",
        "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
        "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
        "\n",
        "        # 損失計算とバックプロパゲーション\n",
        "        loss = self.loss_fn(current_q_values.squeeze(1), target_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # εを減少させる\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        torch.save(self.model.state_dict(), filepath)\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        self.model.load_state_dict(torch.load(filepath))\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG9n3LF71Kcl"
      },
      "outputs": [],
      "source": [
        "def train_dqn(agent_dict, env, num_iterations, num_episodes_per_iteration):\n",
        "    total_rewards = {agent: 0.0 for agent in env.agents}\n",
        "    rewards = {agent: 0.0 for agent in env.agents}\n",
        "    steps = 0\n",
        "    for iteration in range(num_iterations):\n",
        "        for episode in range(num_episodes_per_iteration):\n",
        "            obs = env.reset()\n",
        "            prev_obs = {agent: obs for agent in env.agents}\n",
        "            prev_action = {agent: None for agent in env.agents}\n",
        "            is_last_reward = False\n",
        "\n",
        "            for agent in env.agent_iter():\n",
        "                obs, total_reward, terminated, truncated, _ = env.last()\n",
        "                done = terminated or truncated\n",
        "\n",
        "                if not is_last_reward:\n",
        "                    rewards = env.rewards.copy()  # 各エージェントの報酬を更新\n",
        "\n",
        "                if prev_action[agent] is not None:\n",
        "                    # 前回行動の結果が今回のループで得られたので、ここで保存できる\n",
        "                    agent_dict[agent].store_experience(\n",
        "                        prev_obs[agent],         # s\n",
        "                        prev_action[agent],      # a\n",
        "                        rewards[agent],      # r (現在のループで得られた報酬)\n",
        "                        obs,                     # s' (次状態)\n",
        "                        float(done)              # done\n",
        "                    )\n",
        "                    # ここでreplayを行う\n",
        "                    if env.step_count % replay_interval == 0:\n",
        "                        for replay_agent in agent_dict.keys():\n",
        "                            agent_dict[replay_agent].replay()\n",
        "\n",
        "                if done:\n",
        "                    action = None  # No action needed if agent is done\n",
        "                    total_rewards[agent] += total_reward\n",
        "                    steps += env.step_count\n",
        "                    print(f\"Episode {episode} finished for agent {agent} with reward {total_reward}, {rewards[agent]}, steps {env.step_count}\")\n",
        "                    is_last_reward = True\n",
        "                else:\n",
        "                    action = agent_dict[agent].act(obs)\n",
        "\n",
        "                env.step(action)\n",
        "\n",
        "                prev_obs[agent] = obs  # 次の状態を更新\n",
        "                prev_action[agent] = action  # 次の行動を更新\n",
        "\n",
        "        # ログ出力\n",
        "        if iteration % update_target_steps == 0:\n",
        "            print(f\"Iteration {iteration}: \" + \", \".join([f\"{a}: {r / update_target_steps:.2f}\" for a, r in total_rewards.items()]), steps / update_target_steps)\n",
        "            total_rewards = {agent: 0.0 for agent in total_rewards.keys()}\n",
        "            steps = 0\n",
        "\n",
        "        # ターゲットネットワーク更新\n",
        "        if iteration % update_target_steps == 0:\n",
        "            for agent in agent_dict.values():\n",
        "                agent.update_target_model()\n",
        "\n",
        "def evaluate_model(agent_dict, eval_env, n_eval_episodes=10):\n",
        "    reward_sums = {agent_name: [] for agent_name in agent_dict.keys()}\n",
        "\n",
        "    for _ in range(n_eval_episodes):\n",
        "        env = eval_env  # 環境がreset可能で、内部状態が共有でないと仮定\n",
        "        env.reset()\n",
        "        episode_rewards = {agent_name: 0.0 for agent_name in agent_dict.keys()}\n",
        "\n",
        "        for agent in env.agent_iter():\n",
        "            obs, reward, termination, truncation, info = env.last()\n",
        "            done = termination or truncation\n",
        "\n",
        "            if done:\n",
        "                action = None  # 終了したら行動不要\n",
        "            else:\n",
        "                action = agent_dict[agent].act(obs)  # 各エージェントに行動させる\n",
        "\n",
        "            env.step(action)\n",
        "            episode_rewards[agent] += reward  # 各agentごとに報酬を記録\n",
        "\n",
        "        for agent_name in reward_sums:\n",
        "            reward_sums[agent_name].append(episode_rewards[agent_name])\n",
        "\n",
        "    # 統計量（平均・標準偏差）を返す\n",
        "    mean_std_rewards = {\n",
        "        agent: (np.mean(rewards), np.std(rewards))\n",
        "        for agent, rewards in reward_sums.items()\n",
        "    }\n",
        "\n",
        "    return mean_std_rewards\n",
        "\n",
        "def save_dqn(agent_dict, base_path = \"models\"):\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    for agent_name, agent in agent_dict.items():\n",
        "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
        "        agent.save_model(filepath)\n",
        "\n",
        "def load_dqn(env, agents = [\"cat\", \"toy\"] , base_path = \"models\"):\n",
        "    agent_dict = {}\n",
        "    for agent_name in agents:\n",
        "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
        "        agent = DQNAgent(agent_name, env)\n",
        "        agent.load_model(filepath)\n",
        "        agent_dict[agent_name] = agent\n",
        "    return agent_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLgDfZG71Kcl"
      },
      "outputs": [],
      "source": [
        "# エージェントの作成\n",
        "agent_dict = {\n",
        "    agent_name: DQNAgent(agent_name, env_learning)\n",
        "    for agent_name in env_learning.agents\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xTPuUOp1Kcm"
      },
      "outputs": [],
      "source": [
        "# 学習\n",
        "train_dqn(agent_dict, env_learning, num_iterations, num_episodes_per_iteration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFi-1ElS1Kcm"
      },
      "outputs": [],
      "source": [
        "# 評価用環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
        "env_eval = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデル評価\n",
        "mean_std_rewards = evaluate_model(agent_dict, env_eval, n_eval_episodes=1)\n",
        "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nxtVPpX1Kcm"
      },
      "outputs": [],
      "source": [
        "# モデルの保存\n",
        "save_dqn(agent_dict, \"models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hryQLSqy1Kcm"
      },
      "outputs": [],
      "source": [
        "# 評価用環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
        "env_eval = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデルのロード\n",
        "loaded_model = load_dqn(env_eval, [\"cat\", \"toy\"], \"models\")\n",
        "\n",
        "# ロードしたモデルの評価\n",
        "mean_std_rewards = evaluate_model(loaded_model, env_eval, n_eval_episodes=10)\n",
        "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYpfaqP1Kcm"
      },
      "outputs": [],
      "source": [
        "# 入力の2つのTensorを結合\n",
        "toy = torch.randn(1, 2)\n",
        "cat = torch.randn(1, 2)\n",
        "concat_input = torch.cat([toy, cat], dim=1)  # shape: (1, 4)\n",
        "\n",
        "# エクスポート対象モデル（例: policyネットワーク）\n",
        "# dummyの環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
        "env_dummy = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデルのロード\n",
        "loaded_model = load_dqn(env_dummy, [\"cat\", \"toy\"], \"models\")\n",
        "policy_net = loaded_model[\"cat\"].model  # catエージェントのポリシーネットワークを取得\n",
        "\n",
        "# ONNXエクスポート\n",
        "torch.onnx.export(\n",
        "    policy_net,\n",
        "    concat_input,  # ← dictではなく単一Tensor\n",
        "    \"cat_dqn_policy.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    input_names=[\"obs\"],\n",
        "    output_names=[\"q_values\"],\n",
        "    dynamic_axes={\n",
        "        \"obs\": {0: \"batch_size\"},\n",
        "        \"q_values\": {0: \"batch_size\"}\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LhUSKTU1Kcm"
      },
      "outputs": [],
      "source": [
        "# 環境のクローズ\n",
        "env_learning.close()\n",
        "env_eval.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA0ZUWa81Kcn"
      },
      "source": [
        "# 学習済みモデルの使用 (play.py)\n",
        "\n",
        "このセルでは、学習済みのモデルをロードし、`CartPole-v1`環境でエージェントがどのように行動するかを観察します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF-RjAvP1Kcn"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "import time\n",
        "from cat_toy_env import CatToyEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ6R6VHI1Kcn"
      },
      "outputs": [],
      "source": [
        "env_kwargs=dict(render_mode=\"\", max_steps=1000, cat_speed = 2)\n",
        "\n",
        "# 環境の作成\n",
        "env = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデルのロード\n",
        "model_playing = DQN.load(\"cat_dqn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0zx04mO1Kcn"
      },
      "outputs": [],
      "source": [
        "# エピソードの実行\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action, _states = model_playing.predict(obs, deterministic=True)\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    print(\"観測:\", obs)\n",
        "    done = terminated or truncated\n",
        "    env.render()  # 環境の描画\n",
        "    #time.sleep(0.001) # 0.01秒待機"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySt54n9F1Kcn"
      },
      "outputs": [],
      "source": [
        "# 環境のクローズ\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVOqvICf1Kcn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}