{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK0o1IZFtA7V"
      },
      "source": [
        "# Google Colab用セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvhNNBEVCIxH"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/nekoneko02/cat-brain.git\n",
        "%cd cat-brain\n",
        "!git checkout origin/cnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkOy-su0Gv80"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!mv /content/cat-brain /content/cat_brain\n",
        "!mv /content/cat_brain/cat-dqn /content/cat_brain/cat_dqn\n",
        "!sed -i 's|\\.\\./cat-game/config/common\\.json|/content/cat_brain/cat-game/config/common.json|g' /content/cat_brain/cat_dqn/cat_toy_env.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1AXujtu1Kci"
      },
      "source": [
        "# 強化学習モデルの学習 (main.py)\n",
        "\n",
        "このセルでは、DQNアルゴリズムを用いて、`CartPole-v1`環境でモデルを学習させます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkqesDnh1Kcg"
      },
      "outputs": [],
      "source": [
        "!apt install cmake swig zlib1g-dev\n",
        "%pip install torch torchvision\n",
        "%pip install numpy onnx\n",
        "%pip install pettingzoo[all]\n",
        "%pip install torchrl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz-KT21E1Kcj"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.test import api_test\n",
        "from cat_toy_env import CatToyEnv\n",
        "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
        "\n",
        "# 1個だけ環境を作る（並列ではなく）\n",
        "env = CatToyEnv(**env_kwargs)\n",
        "api_test(env, num_cycles=1000, verbose_progress=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FZFULGI1Kcj"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "\n",
        "from cat_toy_env import CatToyEnv\n",
        "#from cat_brain.cat_dqn.cat_toy_env import CatToyEnv # Google Colab用\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchrl.data import PrioritizedReplayBuffer, LazyTensorStorage\n",
        "from torchrl.data.replay_buffers.samplers import PrioritizedSampler\n",
        "from torchrl.modules import NoisyLinear\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import cat_toy_env\n",
        "\n",
        "# モジュールを再読み込み\n",
        "importlib.reload(cat_toy_env)\n",
        "\n",
        "# クラスを再インポート\n",
        "from cat_toy_env import CatToyEnv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmXaM6bd1Kck"
      },
      "outputs": [],
      "source": [
        "num_iterations = 60\n",
        "num_episodes_per_iteration = 1\n",
        "num_steps_per_episode = 100000\n",
        "# num_epoches = 1\n",
        "# num_replays_per_episode = num_epoches * num_episodes_per_iteration * num_steps_per_episode\n",
        "update_target_steps = 10\n",
        "replay_interval = 6\n",
        "buffer_size = 10000\n",
        "batch_size = 64\n",
        "sequence_length = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kecc2h61Kck"
      },
      "outputs": [],
      "source": [
        "env_kwargs=dict(render_mode=None, max_steps = num_steps_per_episode)\n",
        "# 1個だけ環境を作る\n",
        "env_preview = CatToyEnv(**env_kwargs)\n",
        "\n",
        "obs = env_preview.reset()\n",
        "\n",
        "# 観測のshapeを確認\n",
        "print(\"観測の形:\", obs)\n",
        "print(\"観測の中身:\", obs)\n",
        "# 学習用環境\n",
        "env_learning = CatToyEnv(**env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UXyofpi1Kck"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, rnn_hidden_dim=64):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # RNN層\n",
        "        self.rnn = nn.GRU(input_dim, rnn_hidden_dim, batch_first=True)\n",
        "\n",
        "        # 特徴抽出層\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Linear(64, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 状態価値関数 V(s)\n",
        "        self.value_stream = nn.Sequential(\n",
        "            NoisyLinear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            NoisyLinear(128, 1)\n",
        "        )\n",
        "\n",
        "        # アドバンテージ関数 A(s, a)\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            NoisyLinear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            NoisyLinear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        # RNNの処理\n",
        "        x, hidden_state = self.rnn(x, hidden_state)\n",
        "        x = x[:, -1, :]  # 最後の出力のみを使用\n",
        "\n",
        "        # 特徴抽出\n",
        "        x = self.feature(x)\n",
        "        value = self.value_stream(x)\n",
        "        advantage = self.advantage_stream(x)\n",
        "\n",
        "        # Q値の計算\n",
        "        q = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
        "        return q, hidden_state\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.agent_name = agent_name\n",
        "        self.action_space = env.action_spaces[self.agent_name]\n",
        "        self.state_shape = env.observation_spaces[self.agent_name].shape[0]\n",
        "        \n",
        "        self.model = DQN(self.state_shape, self.action_space.n).to(device)\n",
        "        self.target_model = DQN(self.state_shape, self.action_space.n).to(device)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((\n",
        "            state,\n",
        "            action,\n",
        "            reward,\n",
        "            next_state,\n",
        "            done\n",
        "        ))\n",
        "\n",
        "    def act(self, state, hidden_state=None):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return self.action_space.sample(), hidden_state\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, state_dim]\n",
        "        q_values, hidden_state = self.model(state, hidden_state)\n",
        "        return torch.argmax(q_values).item(), hidden_state\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        batch = list(zip(*batch))  # tuple of lists\n",
        "\n",
        "        states = torch.FloatTensor(np.stack(batch[0])).to(device)\n",
        "        actions = torch.LongTensor(batch[1]).to(device)\n",
        "        rewards = torch.FloatTensor(batch[2]).to(device)\n",
        "        next_states = torch.FloatTensor(np.stack(batch[3])).to(device)\n",
        "        dones = torch.FloatTensor(batch[4]).to(device)\n",
        "\n",
        "        # RNNの初期隠れ状態\n",
        "        hidden_state = None\n",
        "\n",
        "        # Q値の計算\n",
        "        q_values, _ = self.model(states, hidden_state)\n",
        "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # ターゲットQ値の計算\n",
        "        next_q_values, _ = self.target_model(next_states, hidden_state)\n",
        "        max_next_q_values = next_q_values.max(1)[0]\n",
        "        target_q_values = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
        "\n",
        "        # 損失計算と学習\n",
        "        loss = self.loss_fn(q_values, target_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ε減少\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        torch.save(self.model.state_dict(), filepath)\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        self.model.load_state_dict(torch.load(filepath))\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG9n3LF71Kcl"
      },
      "outputs": [],
      "source": [
        "def train_dqn(agent_dict, env, num_iterations, num_episodes_per_iteration):\n",
        "    total_rewards = {agent: 0.0 for agent in env.agents}\n",
        "    rewards = {agent: 0.0 for agent in env.agents}\n",
        "    steps = 0\n",
        "    for iteration in range(num_iterations):\n",
        "        for episode in range(num_episodes_per_iteration):\n",
        "            obs = env.reset()\n",
        "            seq_obs = {agent: deque(maxlen=sequence_length+1) for agent in env.agents} # (len(agents), sequence_length, state_dim)\n",
        "            for agent in env.agents:\n",
        "                for _ in range(sequence_length+1):\n",
        "                    seq_obs[agent].append(obs)\n",
        "            prev_action = {agent: None for agent in env.agents}\n",
        "            prev_total_reward = {agent: 0.0 for agent in env.agents}\n",
        "\n",
        "            for agent in env.agent_iter():\n",
        "                if agent == \"dummy\":\n",
        "                    # dummyエージェントは行動しない\n",
        "                    action = None\n",
        "                    env.step(action)\n",
        "                    continue\n",
        "\n",
        "                obs, total_reward, terminated, truncated, _ = env.last()\n",
        "                done = terminated or truncated\n",
        "\n",
        "                if prev_action[agent] is not None:\n",
        "                    seq_obs[agent].append(obs)  # 過去の状態を更新\n",
        "                    list_obs = list(seq_obs[agent])\n",
        "                    # 前回行動の結果が今回のループで得られたので、ここで保存できる\n",
        "                    agent_dict[agent].store_experience(\n",
        "                        list_obs[0:-1],         # s\n",
        "                        prev_action[agent],      # a\n",
        "                        total_reward - prev_total_reward[agent],      # r (現在のループで得られた報酬)\n",
        "                        list_obs[1:],                     # s' (次状態)\n",
        "                        float(terminated)              # done\n",
        "                    )\n",
        "                    # ここでreplayを行う\n",
        "                    if env.step_count % replay_interval == 0:\n",
        "                        for replay_agent in [\"cat\", \"toy\"]:\n",
        "                            agent_dict[replay_agent].replay()\n",
        "\n",
        "                if done or env.step_count % 1000 == 0:\n",
        "                    print(f\"{agent} with  steps {env.step_count}, reward {total_reward - prev_total_reward[agent]: 2f}, action: {prev_action}, state is {obs}\")\n",
        "\n",
        "\n",
        "                if done:\n",
        "                    action = None  # No action needed if agent is done\n",
        "                    #total_rewards[agent] += total_reward\n",
        "                    steps += env.step_count\n",
        "                else:\n",
        "                    action, _ = agent_dict[agent].act(obs)\n",
        "\n",
        "                env.step(action)\n",
        "\n",
        "                prev_action[agent] = action  # 次の行動を更新\n",
        "                prev_total_reward[agent] = total_reward # 次の報酬を更新\n",
        "\n",
        "        # ログ出力\n",
        "        if iteration % update_target_steps == 0:\n",
        "            print(f\"Iteration {iteration}: \" + \", \".join([f\"{a}: {r / update_target_steps:.2f}\" for a, r in total_rewards.items()]), steps / update_target_steps)\n",
        "            #total_rewards = {agent: 0.0 for agent in total_rewards.keys()}\n",
        "            steps = 0\n",
        "\n",
        "        # ターゲットネットワーク更新\n",
        "        if iteration % update_target_steps == 0:\n",
        "            for agent in agent_dict.values():\n",
        "                agent.update_target_model()\n",
        "\n",
        "def evaluate_model(agent_dict, eval_env, n_eval_episodes=10):\n",
        "    reward_sums = {agent_name: [] for agent_name in agent_dict.keys()}\n",
        "\n",
        "    for _ in range(n_eval_episodes):\n",
        "        env = eval_env  # 環境がreset可能で、内部状態が共有でないと仮定\n",
        "        env.reset()\n",
        "        episode_rewards = {agent_name: 0.0 for agent_name in agent_dict.keys()}\n",
        "\n",
        "        for agent in env.agent_iter():\n",
        "            if agent == \"dummy\":\n",
        "                # dummyエージェントは行動しない\n",
        "                action = None\n",
        "                env.step(action)\n",
        "                continue\n",
        "            obs, reward, termination, truncation, info = env.last()\n",
        "            done = termination or truncation\n",
        "\n",
        "            if done:\n",
        "                action = None  # 終了したら行動不要\n",
        "            else:\n",
        "                action, _ = agent_dict[agent].act(obs)  # 各エージェントに行動させる\n",
        "\n",
        "            env.step(action)\n",
        "            episode_rewards[agent] += reward  # 各agentごとに報酬を記録\n",
        "\n",
        "        for agent_name in reward_sums:\n",
        "            reward_sums[agent_name].append(episode_rewards[agent_name])\n",
        "\n",
        "    # 統計量（平均・標準偏差）を返す\n",
        "    mean_std_rewards = {\n",
        "        agent: (np.mean(rewards), np.std(rewards))\n",
        "        for agent, rewards in reward_sums.items()\n",
        "    }\n",
        "\n",
        "    return mean_std_rewards\n",
        "\n",
        "def save_dqn(agent_dict, base_path = \"models\"):\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    for agent_name, agent in agent_dict.items():\n",
        "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
        "        agent.save_model(filepath)\n",
        "\n",
        "def load_dqn(env, agents = [\"cat\", \"toy\"] , base_path = \"models\"):\n",
        "    agent_dict = {}\n",
        "    for agent_name in agents:\n",
        "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
        "        agent = DQNAgent(agent_name, env)\n",
        "        agent.load_model(filepath)\n",
        "        agent_dict[agent_name] = agent\n",
        "    return agent_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLgDfZG71Kcl"
      },
      "outputs": [],
      "source": [
        "# エージェントの作成\n",
        "agent_dict = {\n",
        "    agent_name: DQNAgent(agent_name, env_learning)\n",
        "    for agent_name in env_learning.agents\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xTPuUOp1Kcm"
      },
      "outputs": [],
      "source": [
        "# 学習\n",
        "train_dqn(agent_dict, env_learning, num_iterations, num_episodes_per_iteration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFi-1ElS1Kcm"
      },
      "outputs": [],
      "source": [
        "# 評価用環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
        "env_eval = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデル評価\n",
        "mean_std_rewards = evaluate_model(agent_dict, env_eval, n_eval_episodes=1)\n",
        "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nxtVPpX1Kcm"
      },
      "outputs": [],
      "source": [
        "# モデルの保存\n",
        "save_dqn(agent_dict, \"models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6yAWlhoAsEI"
      },
      "outputs": [],
      "source": [
        "# Google Colab用 Artifact保存\n",
        "%cd /content/cat_brain/cat_dqn\n",
        "save_dqn(agent_dict, \"models\")\n",
        "!git config --global user.email \"taka.flemish.giant@gmail.com\"\n",
        "!git config --global user.name \"nekoneko02\"\n",
        "!git pull\n",
        "!git add models/*\n",
        "!git commit -m \"Model保存 from Google Colab\"\n",
        "!git push origin HEAD:google-colab-artifact\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hryQLSqy1Kcm"
      },
      "outputs": [],
      "source": [
        "# 評価用環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
        "env_eval = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデルのロード\n",
        "loaded_model = load_dqn(env_eval, [\"cat\", \"toy\"], \"models\")\n",
        "\n",
        "# ロードしたモデルの評価\n",
        "mean_std_rewards = evaluate_model(loaded_model, env_eval, n_eval_episodes=10)\n",
        "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYpfaqP1Kcm"
      },
      "outputs": [],
      "source": [
        "# 入力の2つのTensorを結合\n",
        "toy = torch.randn(1, 2)\n",
        "cat = torch.randn(1, 2)\n",
        "dum = torch.randn(1, 2)\n",
        "concat_input = torch.cat([toy, cat, dum], dim=1)  # shape: (1, 4)\n",
        "\n",
        "# エクスポート対象モデル（例: policyネットワーク）\n",
        "# dummyの環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
        "env_dummy = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデルのロード\n",
        "loaded_model = load_dqn(env_dummy, [\"cat\", \"toy\", \"dummy\"], \"models\")\n",
        "policy_net = agent_dict[\"cat\"].model  # catエージェントのポリシーネットワークを取得\n",
        "\n",
        "# ONNXエクスポート\n",
        "torch.onnx.export(\n",
        "    policy_net,\n",
        "    concat_input,  # ← dictではなく単一Tensor\n",
        "    \"cat_dqn_policy.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    input_names=[\"obs\"],\n",
        "    output_names=[\"q_values\"],\n",
        "    dynamic_axes={\n",
        "        \"obs\": {0: \"batch_size\"},\n",
        "        \"q_values\": {0: \"batch_size\"}\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LhUSKTU1Kcm"
      },
      "outputs": [],
      "source": [
        "# 環境のクローズ\n",
        "env_learning.close()\n",
        "env_eval.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
