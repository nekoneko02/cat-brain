{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK0o1IZFtA7V"
      },
      "source": [
        "# Google Colab用セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvhNNBEVCIxH"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/nekoneko02/cat-brain.git\n",
        "%cd cat-brain\n",
        "!git checkout origin/cnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkOy-su0Gv80"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!mv /content/cat-brain /content/cat_brain\n",
        "!mv /content/cat_brain/cat-dqn /content/cat_brain/cat_dqn\n",
        "!sed -i 's|\\.\\./cat-game/config/common\\.json|/content/cat_brain/cat-game/config/common.json|g' /content/cat_brain/cat_dqn/cat_toy_env.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1AXujtu1Kci"
      },
      "source": [
        "# 強化学習モデルの学習 (main.py)\n",
        "\n",
        "このセルでは、DQNアルゴリズムを用いて、`CartPole-v1`環境でモデルを学習させます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkqesDnh1Kcg"
      },
      "outputs": [],
      "source": [
        "!apt install cmake swig zlib1g-dev\n",
        "%pip install torch torchvision\n",
        "%pip install numpy onnx\n",
        "%pip install pettingzoo[all]\n",
        "%pip install torchrl\n",
        "%pip install tensordict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz-KT21E1Kcj"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.test import api_test\n",
        "from cat_toy_env import CatToyEnv\n",
        "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
        "\n",
        "# 1個だけ環境を作る（並列ではなく）\n",
        "env = CatToyEnv(**env_kwargs)\n",
        "api_test(env, num_cycles=1000, verbose_progress=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FZFULGI1Kcj"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "\n",
        "from cat_toy_env import CatToyEnv\n",
        "#from cat_brain.cat_dqn.cat_toy_env import CatToyEnv # Google Colab用\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchrl.data import TensorDictPrioritizedReplayBuffer, LazyTensorStorage\n",
        "from tensordict import TensorDict\n",
        "import torchrl.modules as rlnn\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import cat_toy_env\n",
        "\n",
        "# モジュールを再読み込み\n",
        "importlib.reload(cat_toy_env)\n",
        "\n",
        "# クラスを再インポート\n",
        "from cat_toy_env import CatToyEnv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmXaM6bd1Kck"
      },
      "outputs": [],
      "source": [
        "num_iterations = 100\n",
        "num_episodes_per_iteration = 1\n",
        "num_steps_per_episode = 100000\n",
        "# num_epoches = 1\n",
        "# num_replays_per_episode = num_epoches * num_episodes_per_iteration * num_steps_per_episode\n",
        "update_target_steps = 10\n",
        "replay_interval = 6\n",
        "buffer_size = 10000\n",
        "batch_size = 64\n",
        "sequence_length = 1\n",
        "\n",
        "with open('../cat-game/config/common.json', 'r') as f:\n",
        "  config = json.load(f)\n",
        "v_max = config[\"model\"][\"v_max\"]\n",
        "v_min = config[\"model\"][\"v_min\"]\n",
        "num_atoms = config[\"model\"][\"num_atoms\"]\n",
        "hidden_dim = config[\"model\"][\"hidden_size\"]\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kecc2h61Kck"
      },
      "outputs": [],
      "source": [
        "env_kwargs=dict(render_mode=None, max_steps = num_steps_per_episode)\n",
        "# 1個だけ環境を作る\n",
        "env_preview = CatToyEnv(**env_kwargs)\n",
        "\n",
        "obs = env_preview.reset()\n",
        "\n",
        "# 観測のshapeを確認\n",
        "print(\"観測の形:\", obs)\n",
        "print(\"観測の中身:\", obs)\n",
        "# 学習用環境\n",
        "env_learning = CatToyEnv(**env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UXyofpi1Kck"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_atoms=num_atoms, v_min=v_min, v_max=v_max, rnn_hidden_dim=hidden_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.num_atoms = num_atoms\n",
        "        self.v_min = v_min\n",
        "        self.v_max = v_max\n",
        "        self.delta_z = (v_max - v_min) / (num_atoms - 1)\n",
        "        self.z_support = torch.linspace(self.v_min, self.v_max, self.num_atoms)\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # RNN層\n",
        "        self.rnn = nn.GRU(input_dim, rnn_hidden_dim, batch_first=True)\n",
        "\n",
        "        # 特徴抽出層\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.LazyLinear(256),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 状態価値関数 V(s)\n",
        "        self.value_stream = nn.Sequential(\n",
        "            rlnn.NoisyLinear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            rlnn.NoisyLinear(128, num_atoms)\n",
        "        )\n",
        "\n",
        "        # アドバンテージ関数 A(s, a)\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            rlnn.NoisyLinear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            rlnn.NoisyLinear(128, output_dim * num_atoms)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        # RNNの処理\n",
        "        x, hidden_state = self.rnn(x, hidden_state)\n",
        "        x = x[:, -1, :]  # 最後の出力のみを使用\n",
        "\n",
        "        # 特徴抽出\n",
        "        x = self.feature(x)\n",
        "\n",
        "        value = self.value_stream(x).view(-1, 1, self.num_atoms)\n",
        "        advantage = self.advantage_stream(x).view(-1, self.output_dim, self.num_atoms)\n",
        "\n",
        "        # Distributional Q-values\n",
        "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
        "        q_atoms = q_atoms.view(-1, self.output_dim, self.num_atoms)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probabilities = F.softmax(q_atoms, dim=2)\n",
        "        return probabilities, hidden_state # [batch_size, output_dim, num_atoms]\n",
        "\n",
        "    def get_support(self):\n",
        "        return self.z_support\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.995, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.agent_name = agent_name\n",
        "        self.action_space = env.action_spaces[self.agent_name]\n",
        "        self.state_shape = env.observation_spaces[self.agent_name].shape[0]\n",
        "        \n",
        "        self.model = DQN(self.state_shape, self.action_space.n).to(device)\n",
        "        self.target_model = DQN(self.state_shape, self.action_space.n).to(device)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        self.memory = TensorDictPrioritizedReplayBuffer(storage=LazyTensorStorage(buffer_size), alpha=0.6, beta=0.4)\n",
        "        self.batch_size = batch_size\n",
        "        self.update_target_model()\n",
        "        \n",
        "        self.hidden_state = None\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(TensorDict({\n",
        "            'state': torch.FloatTensor(state),\n",
        "            'action': torch.LongTensor([action]),\n",
        "            'reward': torch.FloatTensor([reward]),\n",
        "            'next_state': torch.FloatTensor(next_state),\n",
        "            'done': torch.FloatTensor([done]),\n",
        "            'td_error': 1.0 # 初期の誤差は1に設定\n",
        "        }))\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return self.action_space.sample()\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(device)  # バッチ次元, Sequence次元を追加\n",
        "        probabilities, self.hidden_state = self.model(state, self.hidden_state)  # [batch_size, output_dim, num_atoms], hidden_state\n",
        "\n",
        "        # 各アクションごとに期待Q値を計算\n",
        "        q_values = torch.sum(probabilities * self.model.get_support(), dim=-1)  # [batch_size, output_dim]\n",
        "        return torch.argmax(q_values).item()  # 最大Q値に基づいて行動を選択\n",
        "\n",
        "    def reset_hidden_state(self):\n",
        "        self.hidden_state = None\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch, info = self.memory.sample(self.batch_size, return_info=True)\n",
        "        indices, weights = info['index'], info['_weight']\n",
        "        weights = torch.FloatTensor(weights).to(device)  # Tensorに変換\n",
        "\n",
        "        states = batch['state'].to(device)\n",
        "        actions = batch['action'].to(device).squeeze()\n",
        "        rewards = batch['reward'].to(device).squeeze()\n",
        "        next_states = batch['next_state'].to(device)\n",
        "        dones = batch['done'].to(device).squeeze()\n",
        "\n",
        "        # 現在の分布の取得\n",
        "        probabilities, _ = self.model(states, None)  # [batch_size, num_actions, num_atoms], hidden_state\n",
        "        batch_size, num_actions, num_atoms = probabilities.shape\n",
        "\n",
        "        batch_indices = torch.arange(batch_size, device=device)\n",
        "        # 選択したアクションの分布を取得\n",
        "        selected_probs = probabilities[batch_indices, actions] # [batch_size, num_atoms]\n",
        "\n",
        "        # 次状態の分布の取得\n",
        "        next_probabilities, _ = self.target_model(next_states, None)  # [batch_size, num_actions, num_atoms], hidden_state\n",
        "\n",
        "        # 次状態の期待Q値の計算\n",
        "        next_q_values = torch.sum(next_probabilities * self.model.get_support(), dim=-1)  # [batch_size, num_actions]\n",
        "        next_actions = torch.argmax(next_q_values, dim=1)  # [batch_size]\n",
        "\n",
        "        # 次状態の分布を選択\n",
        "        next_dist = next_probabilities[batch_indices, next_actions]\n",
        "\n",
        "        # Categorical Projection\n",
        "        projected_distribution = self.project_distribution(rewards, dones, next_dist)\n",
        "\n",
        "        # 損失計算 (クロスエントロピー損失)\n",
        "        kl_div = F.kl_div(torch.log(selected_probs + 1e-8), projected_distribution, reduction='none').sum(dim=1)\n",
        "\n",
        "        # 優先度の更新\n",
        "        td_errors = kl_div.detach()\n",
        "        # 優先度のクリッピング\n",
        "        max_priority = 1e3  # 適宜調整\n",
        "        td_errors = torch.clamp(td_errors, min=1.0, max=max_priority)\n",
        "        batch.set(\"td_error\", td_errors)\n",
        "        self.memory.update_tensordict_priority(batch)\n",
        "\n",
        "        # 損失計算（重み適用）\n",
        "        loss = (weights * kl_div).mean()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ε減少\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "    def project_distribution(self, rewards, dones, next_dist):\n",
        "        \"\"\"\n",
        "        Categorical Projection for C51 algorithm.\n",
        "\n",
        "        Args:\n",
        "            rewards (Tensor): [batch_size] - 報酬\n",
        "            dones (Tensor): [batch_size] - 終端フラグ\n",
        "            next_dist (Tensor): [batch_size, num_atoms] - 次状態の分布\n",
        "\n",
        "        Returns:\n",
        "            projected_distribution (Tensor): [batch_size, num_atoms] - プロジェクション後の分布\n",
        "        \"\"\"\n",
        "        batch_size = rewards.size(0)\n",
        "        z_support = self.model.get_support()  # [num_atoms]\n",
        "        num_atoms = z_support.size(0)\n",
        "        \n",
        "        # 各要素の target_z を計算\n",
        "        target_z = rewards.unsqueeze(1) + self.gamma * z_support.unsqueeze(0) * (1 - dones.unsqueeze(1))\n",
        "        target_z = target_z.clamp(min=self.model.v_min, max=self.model.v_max)\n",
        "\n",
        "        # インデックス計算\n",
        "        b = (target_z - self.model.v_min) / self.model.delta_z\n",
        "        l = b.floor().long()\n",
        "        u = b.ceil().long()\n",
        "\n",
        "        # 下限・上限のクリッピング (無効なインデックスを避けるため)\n",
        "        l = l.clamp(0, num_atoms - 1)\n",
        "        u = u.clamp(0, num_atoms - 1)\n",
        "\n",
        "        # 分布の割り当て\n",
        "        offset = torch.linspace(0, (batch_size - 1) * num_atoms, batch_size, device=device).long().unsqueeze(1)\n",
        "\n",
        "        # 出力分布を初期化\n",
        "        projected_distribution = torch.zeros((batch_size, num_atoms), device=device)\n",
        "        \n",
        "        # 下のインデックスに対して割り当て\n",
        "        projected_distribution.view(-1).index_add_(\n",
        "            0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
        "        )\n",
        "\n",
        "        # 上のインデックスに対して割り当て\n",
        "        projected_distribution.view(-1).index_add_(\n",
        "            0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
        "        )\n",
        "\n",
        "        return projected_distribution\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        torch.save(self.model.state_dict(), filepath)\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        self.model.load_state_dict(torch.load(filepath))\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG9n3LF71Kcl"
      },
      "outputs": [],
      "source": [
        "def train_dqn(agent_dict, env, num_iterations, num_episodes_per_iteration):\n",
        "    total_rewards = {agent: 0.0 for agent in env.agents}\n",
        "    steps = 0\n",
        "    for iteration in range(num_iterations):\n",
        "        for episode in range(num_episodes_per_iteration):\n",
        "            obs = env.reset()\n",
        "            seq_obs = {agent: deque(maxlen=sequence_length+1) for agent in env.agents} # (len(agents), sequence_length, state_dim)\n",
        "            prev_action = {agent: None for agent in env.agents}\n",
        "            prev_total_reward = {agent: 0.0 for agent in env.agents}\n",
        "            hidden_states = {agent: None for agent in env.agents} # RNNの隠れ状態を初期化\n",
        "\n",
        "            for agent in env.agent_iter():\n",
        "                if agent == \"dummy\":\n",
        "                    # dummyエージェントは行動しない\n",
        "                    action = None\n",
        "                    env.step(action)\n",
        "                    continue\n",
        "\n",
        "                obs, total_reward, terminated, truncated, _ = env.last()\n",
        "                done = terminated or truncated\n",
        "                seq_obs[agent].append(obs)\n",
        "\n",
        "                if env.step_count/len(env.agents) > sequence_length+1: # 過去の状態を保存するために、sequence_length+1回以上のステップが必要\n",
        "                    list_obs = list(seq_obs[agent])\n",
        "                    # 前回行動の結果が今回のループで得られたので、ここで保存できる\n",
        "                    agent_dict[agent].store_experience(\n",
        "                        list_obs[0:-1],         # s\n",
        "                        prev_action[agent],      # a\n",
        "                        total_reward - prev_total_reward[agent],      # r (現在のループで得られた報酬)\n",
        "                        list_obs[1:],                     # s' (次状態)\n",
        "                        float(terminated)              # done\n",
        "                    )\n",
        "                    # ここでreplayを行う\n",
        "                    if env.step_count % replay_interval == 0:\n",
        "                        for replay_agent in [\"cat\", \"toy\"]:\n",
        "                            agent_dict[replay_agent].replay()\n",
        "\n",
        "                if done or env.step_count % 1000 == 0:\n",
        "                    print(f\"{agent} with  steps {env.step_count}, reward {total_reward - prev_total_reward[agent]: 2f}, action: {prev_action}, state is {obs}\")\n",
        "\n",
        "\n",
        "                if done:\n",
        "                    action = None  # No action needed if agent is done\n",
        "                    total_rewards[agent] += total_reward\n",
        "                    steps += env.step_count\n",
        "                else:\n",
        "                    action = agent_dict[agent].act(obs)\n",
        "                    agent_dict[agent].reset_hidden_state() # 行動を選択するたびにノイズをリセット\n",
        "\n",
        "                env.step(action)\n",
        "\n",
        "                prev_action[agent] = action  # 次の行動を更新\n",
        "                prev_total_reward[agent] = total_reward # 次の報酬を更新\n",
        "\n",
        "        # ログ出力\n",
        "        if iteration % update_target_steps == 0:\n",
        "            print(f\"+++++++ Iteration {iteration}: \" + \", \".join([f\"{a}: {r / update_target_steps:.2f}\" for a, r in total_rewards.items()]), steps / update_target_steps)\n",
        "            total_rewards = {agent: 0.0 for agent in total_rewards.keys()}\n",
        "            steps = 0\n",
        "\n",
        "        # ターゲットネットワーク更新\n",
        "        if iteration % update_target_steps == 0:\n",
        "            for agent in agent_dict.values():\n",
        "                agent.update_target_model()\n",
        "\n",
        "def evaluate_model(agent_dict, eval_env, n_eval_episodes=10):\n",
        "    reward_sums = {agent_name: [] for agent_name in agent_dict.keys()}\n",
        "\n",
        "    for _ in range(n_eval_episodes):\n",
        "        env = eval_env  # 環境がreset可能で、内部状態が共有でないと仮定\n",
        "        env.reset()\n",
        "        episode_rewards = {agent_name: 0.0 for agent_name in agent_dict.keys()}\n",
        "\n",
        "        for agent in env.agent_iter():\n",
        "            if agent == \"dummy\":\n",
        "                # dummyエージェントは行動しない\n",
        "                action = None\n",
        "                env.step(action)\n",
        "                continue\n",
        "            obs, reward, termination, truncation, info = env.last()\n",
        "            done = termination or truncation\n",
        "\n",
        "            if done:\n",
        "                action = None  # 終了したら行動不要\n",
        "            else:\n",
        "                action = agent_dict[agent].act(obs)  # 各エージェントに行動させる\n",
        "                agent_dict[agent].reset_hidden_state()  # 行動を選択するたびにノイズをリセット\n",
        "\n",
        "            env.step(action)\n",
        "            episode_rewards[agent] += reward  # 各agentごとに報酬を記録\n",
        "\n",
        "        for agent_name in reward_sums:\n",
        "            reward_sums[agent_name].append(episode_rewards[agent_name])\n",
        "\n",
        "    # 統計量（平均・標準偏差）を返す\n",
        "    mean_std_rewards = {\n",
        "        agent: (np.mean(rewards), np.std(rewards))\n",
        "        for agent, rewards in reward_sums.items()\n",
        "    }\n",
        "\n",
        "    return mean_std_rewards\n",
        "\n",
        "def save_dqn(agent_dict, base_path = \"models\"):\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    for agent_name, agent in agent_dict.items():\n",
        "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
        "        agent.save_model(filepath)\n",
        "\n",
        "def load_dqn(env, agents = [\"cat\", \"toy\"] , base_path = \"models\"):\n",
        "    agent_dict = {}\n",
        "    for agent_name in agents:\n",
        "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
        "        agent = DQNAgent(agent_name, env)\n",
        "        _ = agent.act(env.reset())\n",
        "        agent.load_model(filepath)\n",
        "        agent_dict[agent_name] = agent\n",
        "    return agent_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLgDfZG71Kcl"
      },
      "outputs": [],
      "source": [
        "# エージェントの作成\n",
        "agent_dict = {\n",
        "    agent_name: DQNAgent(agent_name, env_learning)\n",
        "    for agent_name in env_learning.agents\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xTPuUOp1Kcm"
      },
      "outputs": [],
      "source": [
        "# 学習\n",
        "train_dqn(agent_dict, env_learning, num_iterations, num_episodes_per_iteration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFi-1ElS1Kcm"
      },
      "outputs": [],
      "source": [
        "# 評価用環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=3000)\n",
        "env_eval = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデル評価\n",
        "mean_std_rewards = evaluate_model(agent_dict, env_eval, n_eval_episodes=1)\n",
        "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nxtVPpX1Kcm"
      },
      "outputs": [],
      "source": [
        "# モデルの保存\n",
        "save_dqn(agent_dict, \"models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6yAWlhoAsEI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Google Colab用 Artifact保存\n",
        "%cd /content/cat_brain/cat_dqn\n",
        "save_dqn(agent_dict, \"models\")\n",
        "!git config --global user.email \"taka.flemish.giant@gmail.com\"\n",
        "!git config --global user.name \"nekoneko02\"\n",
        "!git pull\n",
        "!git add models/*\n",
        "!git commit -m \"Model保存 from Google Colab\"\n",
        "!git push origin HEAD:google-colab-artifact\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYpfaqP1Kcm"
      },
      "outputs": [],
      "source": [
        "# 入力の2つのTensorを結合\n",
        "toy = torch.randn(1, 2)\n",
        "cat = torch.randn(1, 2)\n",
        "dum = torch.randn(1, 2)\n",
        "hidden_state = torch.randn(1, 1, 64)\n",
        "concat_input = torch.cat([toy, cat, dum], dim=1).unsqueeze(0)  # shape: (1, 1, obs_dim)\n",
        "\n",
        "# エクスポート対象モデル（例: policyネットワーク）\n",
        "# dummyの環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
        "env_dummy = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデルのロード\n",
        "loaded_model = load_dqn(env_dummy, [\"cat\", \"toy\"], \"models\")\n",
        "policy_net = loaded_model[\"cat\"].model  # catエージェントのポリシーネットワークを取得\n",
        "\n",
        "# ONNX エクスポート\n",
        "torch.onnx.export(\n",
        "    policy_net,\n",
        "    (concat_input, hidden_state),  # RNN用の入力は (入力テンソル, 隠れ状態) とする\n",
        "    \"cat_dqn_policy.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    input_names=[\"obs\", \"hidden_state\"],\n",
        "    output_names=[\"probabilities\", \"next_hidden_state\"],\n",
        "    dynamic_axes={\n",
        "        \"obs\": {0: \"batch_size\"},  # 観測データのバッチ次元を可変に\n",
        "        \"hidden_state\": {0: \"batch_size\"},  # 隠れ状態のバッチ次元を可変に\n",
        "        \"probabilities\": {0: \"batch_size\"},\n",
        "        \"next_hidden_state\": {0: \"batch_size\"}\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_dummy = CatToyEnv(**env_kwargs)\n",
        "obs = torch.FloatTensor(env_dummy.reset()).unsqueeze(0)\n",
        "print(\"obs:\", obs)\n",
        "print(loaded_model[\"cat\"].model(obs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LhUSKTU1Kcm"
      },
      "outputs": [],
      "source": [
        "# 環境のクローズ\n",
        "env_learning.close()\n",
        "env_eval.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
