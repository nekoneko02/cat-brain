{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium stable-baselines3 torch\n",
    "%pip install stable-baselines3[extra]\n",
    "%pip install sb3-contrib\n",
    "%pip install torch torchvision\n",
    "%pip install numpy protobuf onnx onnxruntime\n",
    "%pip install onnx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習モデルの学習 (main.py)\n",
    "\n",
    "このセルでは、DQNアルゴリズムを用いて、`CartPole-v1`環境でモデルを学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import api_test\n",
    "from cat_toy_env import CatToyEnv\n",
    "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
    "\n",
    "# 1個だけ環境を作る（並列ではなく）\n",
    "env = CatToyEnv(**env_kwargs)\n",
    "api_test(env, num_cycles=1000, verbose_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from cat_toy_env import CatToyEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000\n",
    "num_episodes_per_iteration = 1\n",
    "num_steps_per_episode = 1000\n",
    "# num_epoches = 1\n",
    "# num_replays_per_episode = num_epoches * num_episodes_per_iteration * num_steps_per_episode\n",
    "update_target_steps = 10\n",
    "replay_interval = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs=dict(render_mode=None, max_steps = num_steps_per_episode)\n",
    "# 1個だけ環境を作る\n",
    "env_preview = CatToyEnv(**env_kwargs)\n",
    "\n",
    "obs = env_preview.reset()\n",
    "\n",
    "# 観測のshapeを確認\n",
    "print(\"観測の形:\", obs)\n",
    "print(\"観測の中身:\", obs)\n",
    "# 学習用環境\n",
    "env_learning = CatToyEnv(**env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# CNNを使ったQネットワーク\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        c, h, w = input_shape  # (1, H, W)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 2, kernel_size=3, stride=1, padding=1),  # (8, H, W)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                                   # (8, H/2, W/2)\n",
    "\n",
    "            nn.Conv2d(2, 4, kernel_size=3, stride=1, padding=1), # (16, H/2, W/2)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                                    # (16, H/4, W/4),                                   # (8, H/2, W/2)\n",
    "\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)                                  \n",
    "        )\n",
    "\n",
    "        # 畳み込み後の特徴量サイズを計算\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            conv_out = self.conv(dummy_input)\n",
    "            conv_out_size = conv_out.view(1, -1).size(1)\n",
    "            print(\"Conv出力の形:\", conv_out.shape)\n",
    "\n",
    "        # 全結合層\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.agent_name = agent_name  # エージェント名（'cat' または 'toy'）\n",
    "        self.action_space = env.action_spaces[self.agent_name]  # 各エージェントに対応するアクション空間\n",
    "        self.state_shape = env.observation_spaces[self.agent_name].shape  # 例: (1, H, W)\n",
    "        self.model = DQN(self.state_shape, self.action_space.n)\n",
    "        self.target_model = DQN(self.state_shape, self.action_space.n)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 64\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((\n",
    "            np.array(state, dtype=np.float32),\n",
    "            action,\n",
    "            reward,\n",
    "            np.array(next_state, dtype=np.float32),\n",
    "            done\n",
    "        ))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return self.action_space.sample()  # ランダム行動\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # バッチ次元を追加\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()  # 最大Q値に基づいて行動を選択\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Q値の計算\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "        # 損失計算とバックプロパゲーション\n",
    "        loss = self.loss_fn(current_q_values.squeeze(1), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # εを減少させる\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        torch.save(self.model.state_dict(), filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        self.model.load_state_dict(torch.load(filepath))\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent_dict, env, num_iterations, num_episodes_per_iteration):\n",
    "    total_rewards = {agent: 0.0 for agent in env.agents}\n",
    "    rewards = {agent: 0.0 for agent in env.agents}\n",
    "    steps = 0\n",
    "    for iteration in range(num_iterations):\n",
    "        for episode in range(num_episodes_per_iteration):\n",
    "            obs = env.reset()\n",
    "            prev_obs = {agent: obs for agent in env.agents}\n",
    "            prev_action = {agent: None for agent in env.agents}\n",
    "            is_last_reward = False\n",
    "\n",
    "            for agent in env.agent_iter():\n",
    "                obs, total_reward, terminated, truncated, _ = env.last()\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if not is_last_reward:\n",
    "                    rewards = env.rewards.copy()  # 各エージェントの報酬を更新\n",
    "\n",
    "                if prev_action[agent] is not None:\n",
    "                    # 前回行動の結果が今回のループで得られたので、ここで保存できる\n",
    "                    agent_dict[agent].store_experience(\n",
    "                        prev_obs[agent],         # s\n",
    "                        prev_action[agent],      # a\n",
    "                        rewards[agent],      # r (現在のループで得られた報酬)\n",
    "                        obs,                     # s' (次状態)\n",
    "                        float(done)              # done\n",
    "                    )\n",
    "                    # ここでreplayを行う\n",
    "                    if env.step_count % replay_interval == 0:\n",
    "                        for replay_agent in agent_dict.keys():\n",
    "                            agent_dict[replay_agent].replay()\n",
    "\n",
    "                if done:\n",
    "                    action = None  # No action needed if agent is done\n",
    "                    total_rewards[agent] += total_reward\n",
    "                    steps += env.step_count\n",
    "                    print(f\"Episode {episode} finished for agent {agent} with reward {total_reward}, {rewards[agent]}, steps {env.step_count}\")\n",
    "                    is_last_reward = True\n",
    "                else:\n",
    "                    action = agent_dict[agent].act(obs)\n",
    "\n",
    "                env.step(action)\n",
    "\n",
    "                prev_obs[agent] = obs  # 次の状態を更新\n",
    "                prev_action[agent] = action  # 次の行動を更新\n",
    "\n",
    "        # ログ出力\n",
    "        if iteration % update_target_steps == 0:\n",
    "            print(f\"Iteration {iteration}: \" + \", \".join([f\"{a}: {r / update_target_steps:.2f}\" for a, r in total_rewards.items()]), steps / update_target_steps)\n",
    "            total_rewards = {agent: 0.0 for agent in total_rewards.keys()}\n",
    "            steps = 0\n",
    "\n",
    "        # ターゲットネットワーク更新\n",
    "        if iteration % update_target_steps == 0:\n",
    "            for agent in agent_dict.values():\n",
    "                agent.update_target_model()\n",
    "\n",
    "def evaluate_model(agent_dict, eval_env, n_eval_episodes=10):\n",
    "    reward_sums = {agent_name: [] for agent_name in agent_dict.keys()}\n",
    "\n",
    "    for _ in range(n_eval_episodes):\n",
    "        env = eval_env  # 環境がreset可能で、内部状態が共有でないと仮定\n",
    "        env.reset()\n",
    "        episode_rewards = {agent_name: 0.0 for agent_name in agent_dict.keys()}\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            done = termination or truncation\n",
    "\n",
    "            if done:\n",
    "                action = None  # 終了したら行動不要\n",
    "            else:\n",
    "                action = agent_dict[agent].act(obs)  # 各エージェントに行動させる\n",
    "\n",
    "            env.step(action)\n",
    "            episode_rewards[agent] += reward  # 各agentごとに報酬を記録\n",
    "\n",
    "        for agent_name in reward_sums:\n",
    "            reward_sums[agent_name].append(episode_rewards[agent_name])\n",
    "\n",
    "    # 統計量（平均・標準偏差）を返す\n",
    "    mean_std_rewards = {\n",
    "        agent: (np.mean(rewards), np.std(rewards))\n",
    "        for agent, rewards in reward_sums.items()\n",
    "    }\n",
    "\n",
    "    return mean_std_rewards\n",
    "\n",
    "def save_dqn(agent_dict, base_path = \"models\"):\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    for agent_name, agent in agent_dict.items():\n",
    "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
    "        agent.save_model(filepath)\n",
    "\n",
    "def load_dqn(env, agents = [\"cat\", \"toy\"] , base_path = \"models\"):\n",
    "    agent_dict = {}\n",
    "    for agent_name in agents:\n",
    "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
    "        agent = DQNAgent(agent_name, env)\n",
    "        agent.load_model(filepath)\n",
    "        agent_dict[agent_name] = agent\n",
    "    return agent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントの作成\n",
    "agent_dict = {\n",
    "    agent_name: DQNAgent(agent_name, env_learning)\n",
    "    for agent_name in env_learning.agents\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "train_dqn(agent_dict, env_learning, num_iterations, num_episodes_per_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_eval = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデル評価\n",
    "mean_std_rewards = evaluate_model(agent_dict, env_eval, n_eval_episodes=1)\n",
    "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "save_dqn(agent_dict, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_eval = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "loaded_model = load_dqn(env_eval, [\"cat\", \"toy\"], \"models\")\n",
    "\n",
    "# ロードしたモデルの評価\n",
    "mean_std_rewards = evaluate_model(loaded_model, env_eval, n_eval_episodes=10)\n",
    "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力の2つのTensorを結合\n",
    "toy = torch.randn(1, 2)\n",
    "cat = torch.randn(1, 2)\n",
    "concat_input = torch.cat([toy, cat], dim=1)  # shape: (1, 4)\n",
    "\n",
    "# エクスポート対象モデル（例: policyネットワーク）\n",
    "# dummyの環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_dummy = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "loaded_model = load_dqn(env_dummy, [\"cat\", \"toy\"], \"models\")\n",
    "policy_net = loaded_model[\"cat\"].model  # catエージェントのポリシーネットワークを取得\n",
    "\n",
    "# ONNXエクスポート\n",
    "torch.onnx.export(\n",
    "    policy_net,\n",
    "    concat_input,  # ← dictではなく単一Tensor\n",
    "    \"cat_dqn_policy.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    input_names=[\"obs\"],\n",
    "    output_names=[\"q_values\"],\n",
    "    dynamic_axes={\n",
    "        \"obs\": {0: \"batch_size\"},\n",
    "        \"q_values\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境のクローズ\n",
    "env_learning.close()\n",
    "env_eval.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習済みモデルの使用 (play.py)\n",
    "\n",
    "このセルでは、学習済みのモデルをロードし、`CartPole-v1`環境でエージェントがどのように行動するかを観察します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "import time\n",
    "from cat_toy_env import CatToyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs=dict(render_mode=\"\", max_steps=1000, cat_speed = 2)\n",
    "\n",
    "# 環境の作成\n",
    "env = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "model_playing = DQN.load(\"cat_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エピソードの実行\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model_playing.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(\"観測:\", obs)\n",
    "    done = terminated or truncated\n",
    "    env.render()  # 環境の描画\n",
    "    #time.sleep(0.001) # 0.01秒待機"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境のクローズ\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
