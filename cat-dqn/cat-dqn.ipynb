{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK0o1IZFtA7V"
      },
      "source": [
        "# Google Colab用セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvhNNBEVCIxH"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/nekoneko02/cat-brain.git\n",
        "%cd cat-brain\n",
        "!git checkout origin/cnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkOy-su0Gv80"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!mv /content/cat-brain /content/cat_brain\n",
        "!mv /content/cat_brain/cat-dqn /content/cat_brain/cat_dqn\n",
        "!sed -i 's|\\.\\./cat-game/config/common\\.json|/content/cat_brain/cat-game/config/common.json|g' /content/cat_brain/cat_dqn/cat_toy_env.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1AXujtu1Kci"
      },
      "source": [
        "# 強化学習モデルの学習 (main.py)\n",
        "\n",
        "このセルでは、DQNアルゴリズムを用いて、`CartPole-v1`環境でモデルを学習させます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkqesDnh1Kcg"
      },
      "outputs": [],
      "source": [
        "!apt install cmake swig zlib1g-dev\n",
        "%pip install torch torchvision\n",
        "%pip install numpy onnx\n",
        "%pip install pettingzoo[all]\n",
        "%pip install torchrl\n",
        "%pip install tensordict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz-KT21E1Kcj"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.test import api_test\n",
        "from cat_toy_env import CatToyEnv\n",
        "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
        "\n",
        "# 1個だけ環境を作る（並列ではなく）\n",
        "env = CatToyEnv(**env_kwargs)\n",
        "api_test(env, num_cycles=1000, verbose_progress=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FZFULGI1Kcj"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "\n",
        "from cat_toy_env import CatToyEnv\n",
        "#from cat_brain.cat_dqn.cat_toy_env import CatToyEnv # Google Colab用\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchrl.data import TensorDictPrioritizedReplayBuffer, PrioritizedSampler, LazyTensorStorage\n",
        "from tensordict import TensorDict\n",
        "import torchrl.modules as rlnn\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import cat_toy_env\n",
        "\n",
        "# モジュールを再読み込み\n",
        "importlib.reload(cat_toy_env)\n",
        "\n",
        "# クラスを再インポート\n",
        "from cat_toy_env import CatToyEnv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmXaM6bd1Kck"
      },
      "outputs": [],
      "source": [
        "num_iterations = 100\n",
        "num_episodes_per_iteration = 1\n",
        "num_steps_per_episode = 100000\n",
        "# num_epoches = 1\n",
        "# num_replays_per_episode = num_epoches * num_episodes_per_iteration * num_steps_per_episode\n",
        "update_target_steps = 10\n",
        "replay_interval = 6\n",
        "buffer_size = 10000\n",
        "batch_size = 64\n",
        "sequence_length = 5\n",
        "\n",
        "with open('../cat-game/config/common.json', 'r') as f:\n",
        "  config = json.load(f)\n",
        "v_max = config[\"model\"][\"v_max\"]\n",
        "v_min = config[\"model\"][\"v_min\"]\n",
        "num_atoms = config[\"model\"][\"num_atoms\"]\n",
        "hidden_dim = config[\"model\"][\"hidden_size\"]\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kecc2h61Kck"
      },
      "outputs": [],
      "source": [
        "env_kwargs=dict(render_mode=None, max_steps = num_steps_per_episode)\n",
        "# 1個だけ環境を作る\n",
        "env_preview = CatToyEnv(**env_kwargs)\n",
        "\n",
        "obs = env_preview.reset()\n",
        "\n",
        "# 観測のshapeを確認\n",
        "print(\"観測の形:\", obs)\n",
        "print(\"観測の中身:\", obs)\n",
        "# 学習用環境\n",
        "env_learning = CatToyEnv(**env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UXyofpi1Kck"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_atoms=num_atoms, v_min=v_min, v_max=v_max, rnn_hidden_dim=hidden_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.num_atoms = num_atoms\n",
        "        self.v_min = v_min\n",
        "        self.v_max = v_max\n",
        "        self.delta_z = (v_max - v_min) / (num_atoms - 1)\n",
        "        self.z_support = torch.linspace(self.v_min, self.v_max, self.num_atoms)\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        \"\"\"個別実装\"\"\"\n",
        "        self.feature = None\n",
        "\n",
        "        # 状態価値関数 V(s)\n",
        "        self.value_stream = None\n",
        "\n",
        "        # アドバンテージ関数 A(s, a)\n",
        "        self.advantage_stream = None\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        \"\"\"\n",
        "        特徴量抽出部分は個別実装とする\n",
        "        ex)\n",
        "        # 特徴抽出\n",
        "        x = self.feature(x)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probabilities = self._get_probabilities(x)\n",
        "        return probabilities, hidden_state # [batch_size, output_dim, num_atoms]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"forward() method should be implemented in subclasses.\")\n",
        "\n",
        "    def _get_probabilities(self, x):\n",
        "        value = self.value_stream(x).view(-1, 1, self.num_atoms)\n",
        "        advantage = self.advantage_stream(x).view(-1, self.output_dim, self.num_atoms)\n",
        "\n",
        "        # Distributional Q-values\n",
        "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
        "        q_atoms = q_atoms.view(-1, self.output_dim, self.num_atoms)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probabilities = F.softmax(q_atoms, dim=2)\n",
        "        return probabilities\n",
        "\n",
        "    def get_support(self):\n",
        "        return self.z_support\n",
        "\n",
        "class SequenceTensorDictPrioritizedReplayBuffer(TensorDictPrioritizedReplayBuffer):\n",
        "    def __init__(self, storage, max_capacity, alpha=0.6, beta=0.4, sequence_length=5):\n",
        "        sampler = PrioritizedSampler(max_capacity = max_capacity, alpha=alpha, beta=beta)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        super().__init__(storage=storage, alpha=alpha, beta=beta, )\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def sample(self, batch_size, sequence_length=None, return_info=True):\n",
        "        sequence_length = sequence_length or self.sequence_length\n",
        "\n",
        "        # サンプリング時の優先度に基づくインデックスを取得\n",
        "        indices, info = self._sampler.sample(self._storage, batch_size)\n",
        "        info['index'] = indices\n",
        "\n",
        "        # シーケンスの開始インデックスを計算\n",
        "        start_indices = indices - (sequence_length - 1)\n",
        "        start_indices = start_indices.clamp(min=0)\n",
        "\n",
        "        # シーケンスの収集をループからバッチ処理に変更\n",
        "        indices = torch.arange(sequence_length).unsqueeze(0) + start_indices.unsqueeze(1)  # [batch_size, sequence_length]\n",
        "\n",
        "        batch_data = self._storage.get(indices.flatten())\n",
        "        batch_data = batch_data.view(batch_size, sequence_length, *batch_data.shape[1:])\n",
        "        # バッチデータを取得時に直接デバイスに転送\n",
        "        batch_data = batch_data.to(device)\n",
        "    \n",
        "        return batch_data, info\n",
        "\n",
        "    def update_priorities(self, indices, td_errors):\n",
        "        td_errors = td_errors.view(len(indices), -1).max(dim=1)[0]\n",
        "        # update_prioritiesで直接更新\n",
        "        self._sampler.update_priorities(indices, td_errors.detach().cpu().numpy())\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.995, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, sequence_length = 5):\n",
        "        self.agent_name = agent_name\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = learning_rate\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        \"\"\"個別\"\"\"\n",
        "        self.memory = None\n",
        "        self.action_space = None\n",
        "        self.state_shape = None\n",
        "        self.model = None\n",
        "        self.target_model = None\n",
        "        self.optimizer = None\n",
        "        #self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(TensorDict({\n",
        "            'state': torch.FloatTensor(state),\n",
        "            'action': torch.LongTensor([action]),\n",
        "            'reward': torch.FloatTensor([reward]),\n",
        "            'next_state': torch.FloatTensor(next_state),\n",
        "            'done': torch.FloatTensor([done]),\n",
        "            'td_error': 1.0 # 初期の誤差は1に設定\n",
        "        }))\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return self.action_space.sample()\n",
        "        probabilities = self._call_model_for_act(state)  # [batch_size, output_dim, num_atoms], hidden_state\n",
        "        # 各アクションごとに期待Q値を計算\n",
        "        q_values = torch.sum(probabilities * self.model.get_support(), dim=-1)  # [batch_size, output_dim]\n",
        "        return torch.argmax(q_values).item()  # 最大Q値に基づいて行動を選択\n",
        "\n",
        "    def reset_hidden_state(self):\n",
        "        self.hidden_state = None\n",
        "\n",
        "    def _call_model_for_act(self, state):\n",
        "        \"\"\"\n",
        "        モデルを呼び出して確率を取得する関数\n",
        "        Agent毎のmodelのin, outの差を吸収するために実装する\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"_call_model_for_act() method should be implemented in subclasses.\")\n",
        "    def _call_model(self, state):\n",
        "        \"\"\"\n",
        "        モデルを呼び出して確率を取得する関数\n",
        "        Agent毎のmodelのin, outの差を吸収するために実装する\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"_call_model() method should be implemented in subclasses.\")\n",
        "    def _call_target_model(self, state):\n",
        "        \"\"\"\n",
        "        モデルを呼び出して確率を取得する関数\n",
        "        Agent毎のmodelのin, outの差を吸収するために実装する\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"_call_target_model() method should be implemented in subclasses.\")\n",
        "\n",
        "    def _get_sarsa(self, batch_size, return_info=True):\n",
        "        \"\"\"\n",
        "        サンプリングした経験を取得する関数\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"_call_model() method should be implemented in subclasses.\")\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones, info = self._get_sarsa(self.batch_size)\n",
        "        indices, weights = info['index'], info['_weight']\n",
        "        weights = torch.FloatTensor(weights).to(device)  # Tensorに変換\n",
        "\n",
        "        # 現在の分布の取得\n",
        "        probabilities = self._call_model(states)  # [batch_size, num_actions, num_atoms], hidden_state\n",
        "        batch_size, num_actions, num_atoms = probabilities.shape\n",
        "\n",
        "        batch_indices = torch.arange(batch_size, device=device)\n",
        "        # 選択したアクションの分布を取得\n",
        "        selected_probs = probabilities[batch_indices, actions] # [batch_size, num_atoms]\n",
        "\n",
        "        # 次状態の分布の取得\n",
        "        next_probabilities = self._call_target_model(next_states)  # [batch_size, num_actions, num_atoms], hidden_state\n",
        "\n",
        "        # 次状態の期待Q値の計算\n",
        "        next_q_values = torch.sum(next_probabilities * self.model.get_support(), dim=-1)  # [batch_size, num_actions]\n",
        "        next_actions = torch.argmax(next_q_values, dim=1)  # [batch_size]\n",
        "\n",
        "        # 次状態の分布を選択\n",
        "        next_dist = next_probabilities[batch_indices, next_actions]\n",
        "\n",
        "        # Categorical Projection\n",
        "        projected_distribution = self.project_distribution(rewards, dones, next_dist)\n",
        "\n",
        "        # 損失計算 (クロスエントロピー損失)\n",
        "        kl_div = F.kl_div(torch.log(selected_probs + 1e-8), projected_distribution, reduction='none').sum(dim=1)\n",
        "\n",
        "        # 優先度の更新\n",
        "        td_errors = kl_div.detach()\n",
        "        # 優先度のクリッピング\n",
        "        max_priority = 1e3  # 適宜調整\n",
        "        td_errors = torch.clamp(td_errors, min=1.0, max=max_priority)\n",
        "        self.memory.update_priority(indices, td_errors)\n",
        "\n",
        "        # 損失計算（重み適用）\n",
        "        loss = (weights * kl_div).mean()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ε減少\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "    def project_distribution(self, rewards, dones, next_dist):\n",
        "        \"\"\"\n",
        "        Categorical Projection for C51 algorithm.\n",
        "\n",
        "        Args:\n",
        "            rewards (Tensor): [batch_size] - 報酬\n",
        "            dones (Tensor): [batch_size] - 終端フラグ\n",
        "            next_dist (Tensor): [batch_size, num_atoms] - 次状態の分布\n",
        "\n",
        "        Returns:\n",
        "            projected_distribution (Tensor): [batch_size, num_atoms] - プロジェクション後の分布\n",
        "        \"\"\"\n",
        "        batch_size = rewards.size(0)\n",
        "        z_support = self.model.get_support()  # [num_atoms]\n",
        "        num_atoms = z_support.size(0)\n",
        "        \n",
        "        # 各要素の target_z を計算\n",
        "        target_z = rewards.unsqueeze(1) + self.gamma * z_support.unsqueeze(0) * (1 - dones.unsqueeze(1))\n",
        "        target_z = target_z.clamp(min=self.model.v_min, max=self.model.v_max)\n",
        "\n",
        "        # インデックス計算\n",
        "        b = (target_z - self.model.v_min) / self.model.delta_z\n",
        "        l = b.floor().long()\n",
        "        u = b.ceil().long()\n",
        "\n",
        "        # 下限・上限のクリッピング (無効なインデックスを避けるため)\n",
        "        l = l.clamp(0, num_atoms - 1)\n",
        "        u = u.clamp(0, num_atoms - 1)\n",
        "\n",
        "        # 分布の割り当て\n",
        "        offset = torch.linspace(0, (batch_size - 1) * num_atoms, batch_size, device=device).long().unsqueeze(1)\n",
        "\n",
        "        # 出力分布を初期化\n",
        "        projected_distribution = torch.zeros((batch_size, num_atoms), device=device)\n",
        "        \n",
        "        # 下のインデックスに対して割り当て\n",
        "        projected_distribution.view(-1).index_add_(\n",
        "            0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
        "        )\n",
        "\n",
        "        # 上のインデックスに対して割り当て\n",
        "        projected_distribution.view(-1).index_add_(\n",
        "            0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
        "        )\n",
        "\n",
        "        return projected_distribution\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        torch.save(self.model.state_dict(), filepath)\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        self.model.load_state_dict(torch.load(filepath))\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CatDQN(DQN):\n",
        "    def __init__(self, input_dim, output_dim, num_atoms=num_atoms, v_min=v_min, v_max=v_max, rnn_hidden_dim=hidden_dim):\n",
        "        super(CatDQN, self).__init__(input_dim, output_dim, num_atoms=num_atoms, v_min=v_min, v_max=v_max, rnn_hidden_dim=rnn_hidden_dim)\n",
        "        # RNN層\n",
        "        self.rnn = nn.GRU(input_dim, rnn_hidden_dim, batch_first=True)\n",
        "\n",
        "        # 特徴抽出層\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.LazyLinear(256),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.value_stream = nn.Sequential(\n",
        "            rlnn.NoisyLinear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            rlnn.NoisyLinear(128, num_atoms)\n",
        "        )\n",
        "\n",
        "        # アドバンテージ関数 A(s, a)\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            rlnn.NoisyLinear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            rlnn.NoisyLinear(128, output_dim * num_atoms)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        # RNNの処理\n",
        "        x, hidden_state = self.rnn(x, hidden_state)\n",
        "        x = x[:, -1, :]  # 最後の出力のみを使用\n",
        "\n",
        "        # 特徴抽出\n",
        "        x = self.feature(x)\n",
        "\n",
        "        probabilities = self._get_probabilities(x)\n",
        "        return probabilities, hidden_state # [batch_size, output_dim, num_atoms]\n",
        "\n",
        "class CatDQNAgent(DQNAgent):\n",
        "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.995, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        super().__init__(agent_name, env, learning_rate, gamma, epsilon, epsilon_min, epsilon_decay)\n",
        "        self.memory = SequenceTensorDictPrioritizedReplayBuffer(\n",
        "            storage=LazyTensorStorage(buffer_size),\n",
        "            max_capacity = buffer_size,\n",
        "            alpha=0.6,\n",
        "            beta=0.4,\n",
        "            sequence_length=self.sequence_length)\n",
        "        self.hidden_state = None\n",
        "        self.seq_obs = deque(maxlen=self.sequence_length)\n",
        "        \n",
        "        self.action_space = env.action_spaces[self.agent_name]\n",
        "        self.state_shape = env.observation_spaces[self.agent_name].shape[0]\n",
        "        self.model = CatDQN(self.state_shape, self.action_space.n).to(device)\n",
        "        self.target_model = CatDQN(self.state_shape, self.action_space.n).to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _call_model_for_act(self, state):\n",
        "        self.seq_obs.append(state)\n",
        "        state = torch.FloatTensor(self.seq_obs).unsqueeze(0).to(device)# バッチ次元を追加\n",
        "        probabilities, self.hidden_state = self.model(state, self.hidden_state)  # [batch_size, output_dim, num_atoms], hidden_state\n",
        "        return probabilities\n",
        "    def _call_model(self, state):\n",
        "        probabilities, _ = self.model(state, None)  # [batch_size, output_dim, num_atoms], hidden_state\n",
        "        return probabilities\n",
        "    def _call_target_model(self, state):\n",
        "        probabilities, _ = self.target_model(state, None)  # [batch_size, output_dim, num_atoms], hidden_state\n",
        "        return probabilities\n",
        "\n",
        "    def _get_sarsa(self, batch_size, return_info=True):\n",
        "        batch, info = self.memory.sample(self.batch_size, return_info=return_info)\n",
        "\n",
        "        states = batch['state'].to(device)\n",
        "        actions = batch['action'].to(device).squeeze() # [batch_size, sequence_length] \n",
        "        actions = actions[:, -1]                    # -> [batch_size]\n",
        "        rewards = batch['reward'].to(device).squeeze()\n",
        "        rewards = rewards[:, -1]\n",
        "        next_states = batch['next_state'].to(device)\n",
        "        dones = batch['done'].to(device).squeeze()\n",
        "        dones = dones[:, -1]\n",
        "        return states, actions, rewards, next_states, dones, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToyDQN(DQN):\n",
        "    def __init__(self, input_dim, output_dim, num_atoms=num_atoms, v_min=v_min, v_max=v_max):\n",
        "        super(ToyDQN, self).__init__(input_dim, output_dim, num_atoms=num_atoms, v_min=v_min, v_max=v_max)\n",
        "\n",
        "        # 特徴抽出層\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.LazyLinear(64),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.value_stream = nn.Sequential(\n",
        "            rlnn.NoisyLinear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            rlnn.NoisyLinear(64, num_atoms)\n",
        "        )\n",
        "\n",
        "        # アドバンテージ関数 A(s, a)\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            rlnn.NoisyLinear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            rlnn.NoisyLinear(64, output_dim * num_atoms)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 特徴抽出\n",
        "        x = self.feature(x)\n",
        "\n",
        "        probabilities = self._get_probabilities(x)\n",
        "        return probabilities # [batch_size, output_dim, num_atoms]\n",
        "\n",
        "class ToyDQNAgent(DQNAgent):\n",
        "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.995, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        super().__init__(agent_name, env, learning_rate, gamma, epsilon, epsilon_min, epsilon_decay)\n",
        "        self.memory = TensorDictPrioritizedReplayBuffer(\n",
        "            storage=LazyTensorStorage(buffer_size),\n",
        "            alpha=0.6,\n",
        "            beta=0.4,\n",
        "        )\n",
        "        \n",
        "        self.action_space = env.action_spaces[self.agent_name]\n",
        "        self.state_shape = env.observation_spaces[self.agent_name].shape[0]\n",
        "        self.model = ToyDQN(self.state_shape, self.action_space.n).to(device)\n",
        "        self.target_model = ToyDQN(self.state_shape, self.action_space.n).to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.update_target_model()\n",
        "    def _call_model_for_act(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)# バッチ次元を追加\n",
        "        probabilities = self.model(state)  # [batch_size, output_dim, num_atoms], hidden_state\n",
        "        return probabilities\n",
        "    def _call_model(self, state):\n",
        "        probabilities = self.model(state)\n",
        "        return probabilities\n",
        "    def _call_target_model(self, state):\n",
        "        probabilities = self.target_model(state)\n",
        "        return probabilities\n",
        "    def _get_sarsa(self, batch_size, return_info=True):\n",
        "        batch, info = self.memory.sample(self.batch_size, return_info=return_info)\n",
        "\n",
        "        states = batch['state'].to(device)\n",
        "        actions = batch['action'].to(device).squeeze() # [batch_size, sequence_length]\n",
        "        rewards = batch['reward'].to(device).squeeze()\n",
        "        next_states = batch['next_state'].to(device)\n",
        "        dones = batch['done'].to(device).squeeze()\n",
        "        return states, actions, rewards, next_states, dones, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG9n3LF71Kcl"
      },
      "outputs": [],
      "source": [
        "def train_dqn(agent_dict, env, num_iterations, num_episodes_per_iteration):\n",
        "    total_rewards = {agent: 0.0 for agent in agent_dict.keys()}\n",
        "    steps = 0\n",
        "    for iteration in range(num_iterations):\n",
        "        for episode in range(num_episodes_per_iteration):\n",
        "            obs = env.reset()\n",
        "            prev_obs = {agent: obs for agent in agent_dict.keys()} # 前回の観測を保存\n",
        "            prev_action = {agent: None for agent in agent_dict.keys()}\n",
        "            prev_total_reward = {agent: 0.0 for agent in agent_dict.keys()}\n",
        "\n",
        "            for agent in env.agent_iter():\n",
        "                if agent == \"dummy\":\n",
        "                    # dummyエージェントは行動しない\n",
        "                    action = None\n",
        "                    env.step(action)\n",
        "                    continue\n",
        "\n",
        "                obs, total_reward, terminated, truncated, _ = env.last()\n",
        "                done = terminated or truncated\n",
        "\n",
        "                if prev_action[agent] is not None:\n",
        "                    # 前回行動の結果が今回のループで得られたので、ここで保存できる\n",
        "                    agent_dict[agent].store_experience(\n",
        "                        prev_obs[agent],         # s\n",
        "                        prev_action[agent],      # a\n",
        "                        total_reward - prev_total_reward[agent],      # r (現在のループで得られた報酬)\n",
        "                        obs,                     # s' (次状態)\n",
        "                        float(terminated)              # done\n",
        "                    )\n",
        "                    # ここでreplayを行う\n",
        "                    if env.step_count % replay_interval == 0:\n",
        "                        for replay_agent in agent_dict.keys():\n",
        "                            agent_dict[replay_agent].replay()\n",
        "\n",
        "                if done or env.step_count % 1000 == 0:\n",
        "                    print(f\"{agent} with  steps {env.step_count}, reward {total_reward - prev_total_reward[agent]: 2f}, action: {prev_action}, state is {obs}\")\n",
        "\n",
        "\n",
        "                if done:\n",
        "                    action = None  # No action needed if agent is done\n",
        "                    total_rewards[agent] += total_reward\n",
        "                    steps += env.step_count\n",
        "                else:\n",
        "                    action = agent_dict[agent].act(obs)\n",
        "                    agent_dict[agent].reset_hidden_state() # 行動を選択するたびにノイズをリセット\n",
        "\n",
        "                env.step(action)\n",
        "\n",
        "                prev_action[agent] = action  # 次の行動を更新\n",
        "                prev_total_reward[agent] = total_reward # 次の報酬を更新\n",
        "                prev_obs[agent] = obs\n",
        "\n",
        "        # ログ出力\n",
        "        if iteration % update_target_steps == 0:\n",
        "            print(f\"+++++++ Iteration {iteration}: \" + \", \".join([f\"{a}: {r / update_target_steps:.2f}\" for a, r in total_rewards.items()]), steps / update_target_steps)\n",
        "            total_rewards = {agent: 0.0 for agent in total_rewards.keys()}\n",
        "            steps = 0\n",
        "\n",
        "        # ターゲットネットワーク更新\n",
        "        if iteration % update_target_steps == 0:\n",
        "            for agent in agent_dict.values():\n",
        "                agent.update_target_model()\n",
        "\n",
        "def evaluate_model(agent_dict, eval_env, n_eval_episodes=10):\n",
        "    reward_sums = {agent_name: [] for agent_name in agent_dict.keys()}\n",
        "\n",
        "    for _ in range(n_eval_episodes):\n",
        "        env = eval_env  # 環境がreset可能で、内部状態が共有でないと仮定\n",
        "        env.reset()\n",
        "        episode_rewards = {agent_name: 0.0 for agent_name in agent_dict.keys()}\n",
        "\n",
        "        for agent in env.agent_iter():\n",
        "            if agent == \"dummy\":\n",
        "                # dummyエージェントは行動しない\n",
        "                action = None\n",
        "                env.step(action)\n",
        "                continue\n",
        "            obs, reward, termination, truncation, info = env.last()\n",
        "            done = termination or truncation\n",
        "\n",
        "            if done:\n",
        "                action = None  # 終了したら行動不要\n",
        "            else:\n",
        "                action = agent_dict[agent].act(obs)  # 各エージェントに行動させる\n",
        "                agent_dict[agent].reset_hidden_state()  # 行動を選択するたびにノイズをリセット\n",
        "\n",
        "            env.step(action)\n",
        "            episode_rewards[agent] += reward  # 各agentごとに報酬を記録\n",
        "\n",
        "        for agent_name in reward_sums:\n",
        "            reward_sums[agent_name].append(episode_rewards[agent_name])\n",
        "\n",
        "    # 統計量（平均・標準偏差）を返す\n",
        "    mean_std_rewards = {\n",
        "        agent: (np.mean(rewards), np.std(rewards))\n",
        "        for agent, rewards in reward_sums.items()\n",
        "    }\n",
        "\n",
        "    return mean_std_rewards\n",
        "\n",
        "def save_dqn(agent_dict, base_path = \"models\"):\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    for agent_name, agent in agent_dict.items():\n",
        "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
        "        agent.save_model(filepath)\n",
        "\n",
        "def load_dqn(env, agents = [\"cat\", \"toy\"] , base_path = \"models\"):\n",
        "    # エージェントの作成\n",
        "    agent_dict = {\n",
        "        \"cat\": CatDQNAgent(\"cat\", env_learning),\n",
        "        \"toy\": ToyDQNAgent(\"toy\", env_learning)\n",
        "    }\n",
        "    for agent_name in agents:\n",
        "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
        "        _ = agent_dict[agent_name].act(env.reset())\n",
        "        agent_dict[agent_name].load_model(filepath)\n",
        "    return agent_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLgDfZG71Kcl"
      },
      "outputs": [],
      "source": [
        "# エージェントの作成\n",
        "agent_dict = {\n",
        "    \"cat\": CatDQNAgent(\"cat\", env_learning),\n",
        "    \"toy\": ToyDQNAgent(\"toy\", env_learning)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xTPuUOp1Kcm"
      },
      "outputs": [],
      "source": [
        "# 学習\n",
        "train_dqn(agent_dict, env_learning, num_iterations, num_episodes_per_iteration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFi-1ElS1Kcm"
      },
      "outputs": [],
      "source": [
        "# 評価用環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=3000)\n",
        "env_eval = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデル評価\n",
        "mean_std_rewards = evaluate_model(agent_dict, env_eval, n_eval_episodes=1)\n",
        "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nxtVPpX1Kcm"
      },
      "outputs": [],
      "source": [
        "# モデルの保存\n",
        "save_dqn(agent_dict, \"models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6yAWlhoAsEI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Google Colab用 Artifact保存\n",
        "%cd /content/cat_brain/cat_dqn\n",
        "save_dqn(agent_dict, \"models\")\n",
        "!git config --global user.email \"taka.flemish.giant@gmail.com\"\n",
        "!git config --global user.name \"nekoneko02\"\n",
        "!git pull\n",
        "!git add models/*\n",
        "!git commit -m \"Model保存 from Google Colab\"\n",
        "!git push origin HEAD:google-colab-artifact\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYpfaqP1Kcm"
      },
      "outputs": [],
      "source": [
        "# 入力の2つのTensorを結合\n",
        "toy = torch.randn(1, 2)\n",
        "cat = torch.randn(1, 2)\n",
        "dum = torch.randn(1, 2)\n",
        "hidden_state = torch.randn(1, 1, 64)\n",
        "concat_input = torch.cat([toy, cat, dum], dim=1).unsqueeze(0)  # shape: (1, 1, obs_dim)\n",
        "\n",
        "# エクスポート対象モデル（例: policyネットワーク）\n",
        "# dummyの環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
        "env_dummy = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデルのロード\n",
        "loaded_model = load_dqn(env_dummy, [\"cat\", \"toy\"], \"models\")\n",
        "policy_net = loaded_model[\"cat\"].model  # catエージェントのポリシーネットワークを取得\n",
        "\n",
        "# ONNX エクスポート\n",
        "torch.onnx.export(\n",
        "    policy_net,\n",
        "    (concat_input, hidden_state),  # RNN用の入力は (入力テンソル, 隠れ状態) とする\n",
        "    \"cat_dqn_policy.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    input_names=[\"obs\", \"hidden_state\"],\n",
        "    output_names=[\"probabilities\", \"next_hidden_state\"],\n",
        "    dynamic_axes={\n",
        "        \"obs\": {0: \"batch_size\"},  # 観測データのバッチ次元を可変に\n",
        "        \"hidden_state\": {0: \"batch_size\"},  # 隠れ状態のバッチ次元を可変に\n",
        "        \"probabilities\": {0: \"batch_size\"},\n",
        "        \"next_hidden_state\": {0: \"batch_size\"}\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_dummy = CatToyEnv(**env_kwargs)\n",
        "obs = torch.FloatTensor(env_dummy.reset()).unsqueeze(0)\n",
        "print(\"obs:\", obs)\n",
        "print(loaded_model[\"cat\"].model(obs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LhUSKTU1Kcm"
      },
      "outputs": [],
      "source": [
        "# 環境のクローズ\n",
        "env_learning.close()\n",
        "env_eval.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
