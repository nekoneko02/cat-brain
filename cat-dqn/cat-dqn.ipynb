{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK0o1IZFtA7V"
      },
      "source": [
        "# Google Colab用セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvhNNBEVCIxH"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/nekoneko02/cat-brain.git\n",
        "%cd cat-brain\n",
        "!git checkout origin/cnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkOy-su0Gv80"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!mv /content/cat-brain /content/cat_brain\n",
        "!mv /content/cat_brain/cat-dqn /content/cat_brain/cat_dqn\n",
        "!sed -i 's|\\.\\./cat-game/config/common\\.json|/content/cat_brain/cat-game/config/common.json|g' /content/cat_brain/cat_dqn/cat_toy_env.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1AXujtu1Kci"
      },
      "source": [
        "# 強化学習モデルの学習 (main.py)\n",
        "\n",
        "このセルでは、DQNアルゴリズムを用いて、`CartPole-v1`環境でモデルを学習させます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkqesDnh1Kcg"
      },
      "outputs": [],
      "source": [
        "!apt install cmake swig zlib1g-dev\n",
        "%pip install torch torchvision\n",
        "%pip install numpy onnx\n",
        "%pip install pettingzoo[all]\n",
        "%pip install torchrl\n",
        "%pip install tensordict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz-KT21E1Kcj"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.test import api_test\n",
        "from cat_toy_env import CatToyEnv\n",
        "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
        "\n",
        "# 1個だけ環境を作る（並列ではなく）\n",
        "env = CatToyEnv(**env_kwargs)\n",
        "api_test(env, num_cycles=1000, verbose_progress=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FZFULGI1Kcj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import cat_toy_env\n",
        "import train\n",
        "import dqn_agent\n",
        "# モジュールを再読み込み\n",
        "importlib.reload(cat_toy_env)\n",
        "importlib.reload(train)\n",
        "importlib.reload(dqn_agent)\n",
        "\n",
        "# クラスを再インポート\n",
        "from cat_toy_env import CatToyEnv\n",
        "from train import train_dqn, evaluate_model\n",
        "from dqn_agent import DQNAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmXaM6bd1Kck"
      },
      "outputs": [],
      "source": [
        "env_kwargs=dict(render_mode=None, chaser= \"cat\", runner = \"toy\", dummy = \"dummy\")\n",
        "# 1個だけ環境を作る\n",
        "env_preview = CatToyEnv(**env_kwargs)\n",
        "env_kwargs=dict(render_mode=None, chaser= \"pre-cat\", runner = \"toy\", dummy = \"dummy\")\n",
        "# 1個だけ環境を作る\n",
        "env_pre_preview = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# 観測のshapeを確認\n",
        "print(\"観測の中身:\", env_preview.reset())\n",
        "\n",
        "# num_epoches = 1\n",
        "# num_replays_per_episode = num_epoches * num_episodes_per_iteration * num_steps_per_episode\n",
        "\n",
        "with open('../cat-game/config/common.json', 'r') as f:\n",
        "  config_file = json.load(f)\n",
        "common_categorical_config={\n",
        "  \"v_max\": config_file[\"model\"][\"v_max\"],\n",
        "  \"v_min\": config_file[\"model\"][\"v_min\"],\n",
        "  \"num_atoms\": config_file[\"model\"][\"num_atoms\"]\n",
        "}\n",
        " \n",
        "config = {\n",
        "  \"pre-train\":{\n",
        "    \"num_iterations\": 50,\n",
        "    \"num_episodes_per_iteration\": 1,\n",
        "    \"num_steps_per_episode\": 10000,\n",
        "    \"update_target_steps\": 5,\n",
        "    \"replay_interval\": 7,\n",
        "    \"batch_size\": 64\n",
        "  },\n",
        "  \"train\":{\n",
        "    \"num_iterations\": 100,\n",
        "    \"num_episodes_per_iteration\": 1,\n",
        "    \"num_steps_per_episode\": 10000,\n",
        "    \"update_target_steps\": 10,\n",
        "    \"replay_interval\": 7,\n",
        "    \"batch_size\": 64\n",
        "  },\n",
        "  \"cat\": {\n",
        "    \"dqn\": {\n",
        "      \"input_dim\": env_preview.observation_spaces[\"cat\"].shape[0],\n",
        "      \"rnn\": {\n",
        "        \"hidden_dim\": config_file[\"model\"][\"hidden_size\"],\n",
        "        \"sequence_length\": config_file[\"model\"][\"sequence_length\"],\n",
        "      },\n",
        "      \"feature\": [256, 256],\n",
        "      \"value_stream\": [128, 1],\n",
        "      \"speed_advantage_stream\": [128, 3],\n",
        "      \"direction_advantage_stream\": [128, 4],\n",
        "      \"categorical\": {\n",
        "        \"v_max\": config_file[\"model\"][\"v_max\"],\n",
        "        \"v_min\": config_file[\"model\"][\"v_min\"],\n",
        "        \"num_atoms\": config_file[\"model\"][\"num_atoms\"]\n",
        "      }\n",
        "    },\n",
        "    \"agent\": {\n",
        "      \"state_shape\": env_preview.observation_spaces[\"cat\"].shape[0],\n",
        "      \"action_space\": env_preview.action_spaces[\"cat\"],\n",
        "      \"buffer\": {\n",
        "        \"size\": 10000,\n",
        "        \"alpha\": 0.6,\n",
        "        \"beta\": 0.4\n",
        "      },\n",
        "      \"learning_rate\": 1e-4,\n",
        "      \"discount_rate\": 0.995\n",
        "    }\n",
        "  },\n",
        "  \"pre-cat\": {\n",
        "    \"dqn\": {\n",
        "      \"input_dim\": env_pre_preview.observation_spaces[\"pre-cat\"].shape[0],\n",
        "      \"rnn\": {\n",
        "        \"hidden_dim\": config_file[\"model\"][\"hidden_size\"],\n",
        "        \"sequence_length\": 5\n",
        "      },\n",
        "      \"feature\": [256, 256],\n",
        "      \"value_stream\": [128, 1],\n",
        "      \"advantage_stream\": [128, env_pre_preview.action_spaces[\"pre-cat\"].n],\n",
        "      \"categorical\": {\n",
        "        \"v_max\": config_file[\"model\"][\"v_max\"],\n",
        "        \"v_min\": config_file[\"model\"][\"v_min\"],\n",
        "        \"num_atoms\": config_file[\"model\"][\"num_atoms\"]\n",
        "      }\n",
        "    },\n",
        "    \"agent\": {\n",
        "      \"state_shape\": env_pre_preview.observation_spaces[\"pre-cat\"].shape[0],\n",
        "      \"action_space\": env_pre_preview.action_spaces[\"pre-cat\"],\n",
        "      \"buffer\": {\n",
        "        \"size\": 10000,\n",
        "        \"alpha\": 0.6,\n",
        "        \"beta\": 0.4\n",
        "      },\n",
        "      \"learning_rate\": 1e-4,\n",
        "      \"discount_rate\": 0.995\n",
        "    }\n",
        "  },\n",
        "  \"toy\": {\n",
        "    \"dqn\": {\n",
        "      \"input_dim\": env_preview.observation_spaces[\"toy\"].shape[0],\n",
        "      \"feature\": [\n",
        "        64,\n",
        "        64\n",
        "      ],\n",
        "      \"value_stream\": [\n",
        "        64,\n",
        "        1\n",
        "      ],\n",
        "      \"advantage_stream\": [\n",
        "        64,\n",
        "        env_preview.action_spaces[\"toy\"].n\n",
        "      ],\n",
        "      \"categorical\": {\n",
        "        \"v_max\": 200,\n",
        "        \"v_min\": 0,\n",
        "        \"num_atoms\": 51\n",
        "      }\n",
        "    },\n",
        "    \"agent\": {\n",
        "      \"state_shape\": env_preview.observation_spaces[\"toy\"].shape[0],\n",
        "      \"action_space\": env_preview.action_spaces[\"toy\"],\n",
        "      \"buffer\": {\n",
        "        \"size\": 10000,\n",
        "        \"alpha\": 0.6,\n",
        "        \"beta\": 0.4\n",
        "      },\n",
        "      \"learning_rate\": 1e-4,\n",
        "      \"discount_rate\": 0.995\n",
        "    }\n",
        "  }\n",
        "}\n",
        "print(env_preview.action_spaces[\"cat\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gamma = 0.995#config[\"cat\"][\"agent\"][\"discount_rate\"]\n",
        "worst_reward = -1 + 0 # 各ステップの最悪の報酬\n",
        "best_reward = -0.1 + -0.3 # 各ステップの良い報酬\n",
        "finish_reward = 20\n",
        "print(\"累積報酬の最小値\", worst_reward / (1-gamma))\n",
        "print(\"良い行動を続けるがクリアしない場合の累積報酬\", best_reward / (1-gamma))\n",
        "print(\"累積報酬の最大値\", finish_reward)\n",
        "print(\"1000ステップ後にクリアする場合の累積報酬の最小値\", worst_reward*(1 - gamma**1000) / (1-gamma) + (gamma ** 1000) * finish_reward)\n",
        "print(\"1000ステップ後にクリアする場合の累積報酬の最大値\", best_reward*(1 - gamma**1000) / (1-gamma) + (gamma ** 1000) * finish_reward)\n",
        "print()\n",
        "print(f\"step: {[i for i in range(0, 1000, 100)]}\")\n",
        "for _gamma in [0.9, 0.99, 0.995, 0.999]:\n",
        "  print(f\"{_gamma}: {[_gamma ** i for i in range(0, 1000, 100)]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kecc2h61Kck"
      },
      "outputs": [],
      "source": [
        "env_kwargs=dict(render_mode=None, max_steps = 100000)\n",
        "# 学習用環境\n",
        "env_learning = CatToyEnv(**env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG9n3LF71Kcl"
      },
      "outputs": [],
      "source": [
        "def save_dqn(agent_dict, base_path = \"models\"):\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    for agent_name, agent in agent_dict.items():\n",
        "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
        "        agent.save_model(filepath)\n",
        "\n",
        "def load_dqn(env, agents = [\"cat\", \"toy\"] , base_path = \"models\"):\n",
        "    # エージェントの作成\n",
        "    agent_dict = {\n",
        "        agent: DQNAgent(\n",
        "            config[agent][\"dqn\"],\n",
        "            config[agent][\"agent\"],\n",
        "            device=device\n",
        "        ) for agent in agents\n",
        "    }\n",
        "    \n",
        "    # LazyLinearを初期化\n",
        "    env.reset()\n",
        "    done_agents = {agent: False for agent in agents}\n",
        "    for agent in env.agent_iter():\n",
        "        if agent not in agents:\n",
        "            env.step(0)\n",
        "            continue\n",
        "        obs, total_reward, terminated, truncated, _ = env.last()\n",
        "        done_agents[agent] = True\n",
        "        env.step(agent_dict[agent].act(obs))\n",
        "        if all(done_agents.values()):\n",
        "            break\n",
        "    # Loadする\n",
        "    for agent in agents:\n",
        "        filepath = os.path.join(base_path, f\"{agent}_model.pth\")\n",
        "        agent_dict[agent].load_model(filepath)\n",
        "    return agent_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLgDfZG71Kcl"
      },
      "outputs": [],
      "source": [
        "# エージェントの作成\n",
        "pre_agents = [\"pre-cat\", \"toy\"]\n",
        "\n",
        "pre_agent_dict = {\n",
        "    agent: DQNAgent(\n",
        "        config[agent][\"dqn\"],\n",
        "        config[agent][\"agent\"],\n",
        "        device=device\n",
        "    ) for agent in pre_agents\n",
        "}\n",
        "\"\"\"# 学習済みモデルを適用する場合\n",
        "env_kwargs=dict(render_mode=None, max_steps = config[\"pre-train\"][\"num_steps_per_episode\"], chaser= \"pre-cat\", runner = \"toy\", dummy = None)\n",
        "env_dummy = CatToyEnv(**env_kwargs)\n",
        "pre_agent_dict = load_dqn(env_dummy, agents=[\"pre-cat\", \"toy\"], base_path=\"pre-models\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xTPuUOp1Kcm"
      },
      "outputs": [],
      "source": [
        "env_kwargs=dict(render_mode=None, max_steps = config[\"train\"][\"num_steps_per_episode\"], chaser= \"pre-cat\", runner = \"toy\", dummy = None)\n",
        "# 学習step1用環境\n",
        "env_learning = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# 学習\n",
        "train_dqn(pre_agent_dict, pre_agents, env_learning, config[\"pre-train\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 評価用環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=3000, chaser= \"pre-cat\", runner = \"toy\", dummy = None)\n",
        "env_eval = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデル評価\n",
        "mean_std_rewards = evaluate_model(pre_agent_dict, env_eval, n_eval_episodes=1)\n",
        "print(f\"mean_reward: {mean_std_rewards['pre-cat']} +/- {mean_std_rewards['toy']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# モデルの保存\n",
        "save_dqn(pre_agent_dict, \"pre-models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# エージェントの作成\n",
        "agents = [\"cat\", \"toy\", \"dummy\"]\n",
        "\n",
        "agent_dict = {\n",
        "    \"cat\": DQNAgent(\n",
        "        config[\"cat\"][\"dqn\"],\n",
        "        config[\"cat\"][\"agent\"],\n",
        "        device=device\n",
        "    )\n",
        "}\n",
        "agent_dict[\"toy\"] = pre_agent_dict[\"toy\"] # 学習済みモデルを適用\n",
        "\n",
        "# 学習済みモデルを適用する場合\n",
        "\"\"\"env_kwargs=dict(render_mode=None, max_steps = config[\"train\"][\"num_steps_per_episode\"], chaser= \"cat\", runner = \"toy\", dummy = None)\n",
        "env_dummy = CatToyEnv(**env_kwargs)\n",
        "agent_dict[\"cat\"]=load_dqn(env_dummy, [\"cat\"], \"models\")[\"cat\"]\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_kwargs=dict(render_mode=None, max_steps = config[\"train\"][\"num_steps_per_episode\"], chaser= \"cat\", runner = \"toy\", dummy = \"dummy\")\n",
        "# 学習step2用環境\n",
        "env_learning = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# 学習\n",
        "train_dqn(agent_dict, [\"cat\"], env_learning, config[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFi-1ElS1Kcm"
      },
      "outputs": [],
      "source": [
        "# 評価用環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=3000)\n",
        "env_eval = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデル評価\n",
        "mean_std_rewards = evaluate_model(agent_dict, env_eval, n_eval_episodes=1)\n",
        "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nxtVPpX1Kcm"
      },
      "outputs": [],
      "source": [
        "# モデルの保存\n",
        "save_dqn(agent_dict, \"models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6yAWlhoAsEI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Google Colab用 Artifact保存\n",
        "%cd /content/cat_brain/cat_dqn\n",
        "save_dqn(agent_dict, \"models\")\n",
        "!git config --global user.email \"taka.flemish.giant@gmail.com\"\n",
        "!git config --global user.name \"nekoneko02\"\n",
        "!git pull\n",
        "!git add models/*\n",
        "!git commit -m \"Model保存 from Google Colab\"\n",
        "!git push origin HEAD:google-colab-artifact\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYpfaqP1Kcm"
      },
      "outputs": [],
      "source": [
        "import dqn_onnx\n",
        "importlib.reload(dqn_onnx)\n",
        "\n",
        "# 入力の2つのTensorを結合\n",
        "toy = torch.randn(1, 2)\n",
        "cat = torch.randn(1, 2)\n",
        "dum = torch.randn(1, 2)\n",
        "hidden_state = torch.randn(1, 1, 64)\n",
        "\n",
        "# 各入力を結合\n",
        "single_input = torch.cat([toy, cat, dum], dim=1)  # shape: (1, obs_dim)\n",
        "\n",
        "# sequence_lengthの長さに拡張\n",
        "concat_input = single_input.repeat(config[\"cat\"][\"dqn\"][\"rnn\"][\"sequence_length\"], 1).unsqueeze(0)  # shape: (1, sequence_length, obs_dim)\n",
        "\n",
        "# エクスポート対象モデル（例: policyネットワーク）\n",
        "# dummyの環境\n",
        "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
        "env_dummy = CatToyEnv(**env_kwargs)\n",
        "\n",
        "# モデルのロード\n",
        "loaded_model = load_dqn(env_dummy, [\"cat\", \"toy\"], \"models\")\n",
        "policy_net = loaded_model[\"cat\"].model  # catエージェントのポリシーネットワークを取得\n",
        "policy_net = dqn_onnx.DQNOnnx(policy_net)\n",
        "\n",
        "# ONNX エクスポート\n",
        "torch.onnx.export(\n",
        "    policy_net,\n",
        "    (concat_input),  # RNN用の入力は (入力テンソル, 隠れ状態) とする\n",
        "    \"cat_dqn_policy.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=12,\n",
        "    input_names=[\"obs\"],\n",
        "    output_names=[\"action_speed\", \"action_direction\", \"q_values_speed\", \"q_values_direction\"],\n",
        "    dynamic_axes={\n",
        "        \"obs\": {0: \"batch_size\"},  # 観測データのバッチ次元を可変に\n",
        "        \"action_speed\": {0: \"batch_size\"},\n",
        "        \"action_direction\": {0: \"batch_size\"},\n",
        "        \"q_values_speed\": {0: \"batch_size\"},\n",
        "        \"q_values_direction\": {0: \"batch_size\"}\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
