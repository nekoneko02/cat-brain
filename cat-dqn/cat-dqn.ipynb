{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium stable-baselines3 torch\n",
    "%pip install stable-baselines3[extra]\n",
    "%pip install sb3-contrib\n",
    "%pip install torch torchvision\n",
    "%pip install numpy protobuf onnx onnxruntime\n",
    "%pip install onnx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習モデルの学習 (main.py)\n",
    "\n",
    "このセルでは、DQNアルゴリズムを用いて、`CartPole-v1`環境でモデルを学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import api_test\n",
    "from cat_toy_env import CatToyEnv\n",
    "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
    "\n",
    "# 1個だけ環境を作る（並列ではなく）\n",
    "env = CatToyEnv(**env_kwargs)\n",
    "api_test(env, num_cycles=1000, verbose_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from cat_toy_env import CatToyEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000\n",
    "num_episodes_per_iteration = 1\n",
    "num_steps_per_episode = 1000\n",
    "# num_epoches = 1\n",
    "# num_replays_per_episode = num_epoches * num_episodes_per_iteration * num_steps_per_episode\n",
    "update_target_steps = 10\n",
    "replay_interval = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "観測の形: [286. 338. 301. 106.]\n",
      "観測の中身: [286. 338. 301. 106.]\n"
     ]
    }
   ],
   "source": [
    "env_kwargs=dict(render_mode=None, max_steps = num_steps_per_episode)\n",
    "# 1個だけ環境を作る\n",
    "env_preview = CatToyEnv(**env_kwargs)\n",
    "\n",
    "obs = env_preview.reset()\n",
    "\n",
    "# 観測のshapeを確認\n",
    "print(\"観測の形:\", obs)\n",
    "print(\"観測の中身:\", obs)\n",
    "# 学習用環境\n",
    "env_learning = CatToyEnv(**env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# 定義：ニューラルネットワーク（Qネットワーク）\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "class DQNAgent:\n",
    "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.agent_name = agent_name  # エージェント名（'cat' または 'toy'）\n",
    "        self.action_space = env.action_spaces[self.agent_name]  # 各エージェントに対応するアクション空間\n",
    "        self.state_space = env.observation_spaces[self.agent_name].shape[0]  # 各エージェントに対応する観察空間\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = DQN(self.state_space, self.action_space.n)  # 各エージェント用のDQNモデル\n",
    "        self.target_model = DQN(self.state_space, self.action_space.n)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 64\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((\n",
    "            np.array(state, dtype=np.float32),\n",
    "            action,\n",
    "            reward,\n",
    "            np.array(next_state, dtype=np.float32),\n",
    "            done\n",
    "        ))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return self.action_space.sample()  # ランダム行動\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # バッチ次元を追加\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()  # 最大Q値に基づいて行動を選択\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Q値の計算\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "        # 損失計算とバックプロパゲーション\n",
    "        loss = self.loss_fn(current_q_values.squeeze(1), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # εを減少させる\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        torch.save(self.model.state_dict(), filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        self.model.load_state_dict(torch.load(filepath))\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent_dict, env, num_iterations, num_episodes_per_iteration):\n",
    "    total_rewards = {agent: 0.0 for agent in env.agents}\n",
    "    rewards = {agent: 0.0 for agent in env.agents}\n",
    "    steps = 0\n",
    "    for iteration in range(num_iterations):\n",
    "        for episode in range(num_episodes_per_iteration):\n",
    "            obs = env.reset()\n",
    "            prev_obs = {agent: obs for agent in env.agents}\n",
    "            prev_action = {agent: None for agent in env.agents}\n",
    "\n",
    "            for agent in env.agent_iter():\n",
    "                obs, total_reward, terminated, truncated, _ = env.last()\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if not done:\n",
    "                    rewards = env.rewards.copy()  # 各エージェントの報酬を更新\n",
    "\n",
    "                if prev_action[agent] is not None:\n",
    "                    # 前回行動の結果が今回のループで得られたので、ここで保存できる\n",
    "                    agent_dict[agent].store_experience(\n",
    "                        prev_obs[agent],         # s\n",
    "                        prev_action[agent],      # a\n",
    "                        rewards[agent],      # r (現在のループで得られた報酬)\n",
    "                        obs,                     # s' (次状態)\n",
    "                        float(done)              # done\n",
    "                    )\n",
    "                    # ここでreplayを行う\n",
    "                    if env.step_count % replay_interval == 0:\n",
    "                        for replay_agent in agent_dict.keys():\n",
    "                            agent_dict[replay_agent].replay()\n",
    "\n",
    "                if done:\n",
    "                    action = None  # No action needed if agent is done\n",
    "                    total_rewards[agent] += total_reward\n",
    "                    steps += env.step_count\n",
    "                    print(f\"Episode {episode} finished for agent {agent} with reward {total_reward}, {rewards[agent]}, steps {env.step_count}\")\n",
    "                else:\n",
    "                    action = agent_dict[agent].act(obs)\n",
    "\n",
    "                env.step(action)\n",
    "\n",
    "                prev_obs[agent] = obs  # 次の状態を更新\n",
    "                prev_action[agent] = action  # 次の行動を更新\n",
    "\n",
    "        # ログ出力\n",
    "        if iteration % update_target_steps == 0:\n",
    "            print(f\"Iteration {iteration}: \" + \", \".join([f\"{a}: {r / update_target_steps:.2f}\" for a, r in total_rewards.items()]), steps / update_target_steps)\n",
    "            total_rewards = {agent: 0.0 for agent in total_rewards.keys()}\n",
    "            steps = 0\n",
    "\n",
    "        # ターゲットネットワーク更新\n",
    "        if iteration % update_target_steps == 0:\n",
    "            for agent in agent_dict.values():\n",
    "                agent.update_target_model()\n",
    "\n",
    "def evaluate_model(agent_dict, eval_env, n_eval_episodes=10):\n",
    "    reward_sums = {agent_name: [] for agent_name in agent_dict.keys()}\n",
    "\n",
    "    for _ in range(n_eval_episodes):\n",
    "        env = eval_env  # 環境がreset可能で、内部状態が共有でないと仮定\n",
    "        env.reset()\n",
    "        episode_rewards = {agent_name: 0.0 for agent_name in agent_dict.keys()}\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            done = termination or truncation\n",
    "\n",
    "            if done:\n",
    "                action = None  # 終了したら行動不要\n",
    "            else:\n",
    "                action = agent_dict[agent].act(obs)  # 各エージェントに行動させる\n",
    "\n",
    "            env.step(action)\n",
    "            episode_rewards[agent] += reward  # 各agentごとに報酬を記録\n",
    "\n",
    "        for agent_name in reward_sums:\n",
    "            reward_sums[agent_name].append(episode_rewards[agent_name])\n",
    "\n",
    "    # 統計量（平均・標準偏差）を返す\n",
    "    mean_std_rewards = {\n",
    "        agent: (np.mean(rewards), np.std(rewards))\n",
    "        for agent, rewards in reward_sums.items()\n",
    "    }\n",
    "\n",
    "    return mean_std_rewards\n",
    "\n",
    "def save_dqn(agent_dict, base_path = \"models\"):\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    for agent_name, agent in agent_dict.items():\n",
    "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
    "        agent.save_model(filepath)\n",
    "\n",
    "def load_dqn(env, agents = [\"cat\", \"toy\"] , base_path = \"models\"):\n",
    "    agent_dict = {}\n",
    "    for agent_name in agents:\n",
    "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
    "        agent = DQNAgent(agent_name, env)\n",
    "        agent.load_model(filepath)\n",
    "        agent_dict[agent_name] = agent\n",
    "    return agent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントの作成\n",
    "agent_dict = {\n",
    "    agent_name: DQNAgent(agent_name, env_learning)\n",
    "    for agent_name in env_learning.agents\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished for agent toy with reward 7273.148186799179, 32.55764119219941, steps 153\n",
      "Episode 0 finished for agent cat with reward -7150.344054964631, -32.55764119219941, steps 153\n",
      "Iteration 0: cat: -715.03, toy: 727.31 30.6\n",
      "Episode 0 finished for agent toy with reward 69768.28556326183, 25.612496949731394, steps 447\n",
      "Episode 0 finished for agent cat with reward -69730.52176804135, -25.612496949731394, steps 447\n",
      "Episode 0 finished for agent toy with reward 7001.0185537378, 34.20526275297414, steps 127\n",
      "Episode 0 finished for agent cat with reward -6912.747553995875, -34.20526275297414, steps 127\n",
      "Episode 0 finished for agent toy with reward 12858.582494748583, 32.7566787083184, steps 211\n",
      "Episode 0 finished for agent cat with reward -12727.50770911104, -32.7566787083184, steps 211\n",
      "Episode 0 finished for agent cat with reward -8985.559558610417, -25.0, steps 132\n",
      "Episode 0 finished for agent toy with reward 8846.318756092694, 25.0, steps 132\n",
      "Episode 0 finished for agent toy with reward 19572.535281062163, 26.627053911388696, steps 271\n",
      "Episode 0 finished for agent cat with reward -19440.548459742055, -26.627053911388696, steps 271\n",
      "Episode 0 finished for agent toy with reward 5017.7845982220415, 36.40054944640259, steps 109\n",
      "Episode 0 finished for agent cat with reward -4888.659875002335, -36.40054944640259, steps 109\n",
      "Episode 0 finished for agent toy with reward 13232.634658981851, 30.479501308256342, steps 179\n",
      "Episode 0 finished for agent cat with reward -13135.745781033818, -30.479501308256342, steps 179\n",
      "Episode 0 finished for agent toy with reward 20902.02052851209, 33.301651610693426, steps 273\n",
      "Episode 0 finished for agent cat with reward -20790.655231221313, -33.301651610693426, steps 273\n",
      "Episode 0 finished for agent cat with reward -37497.68732446602, -35.22782990761707, steps 320\n",
      "Episode 0 finished for agent toy with reward 37309.89353304084, 35.22782990761707, steps 320\n",
      "Episode 0 finished for agent toy with reward 21273.881974978784, 26.90724809414742, steps 305\n",
      "Episode 0 finished for agent cat with reward -21069.69173108402, -26.90724809414742, steps 305\n",
      "Iteration 10: cat: -21517.93, toy: 21578.30 474.8\n",
      "Episode 0 finished for agent toy with reward 50148.332970407355, 38.41874542459709, steps 411\n",
      "Episode 0 finished for agent cat with reward -50084.03734236689, -38.41874542459709, steps 411\n",
      "Episode 0 finished for agent toy with reward 57159.68113507319, 24.413111231467404, steps 435\n",
      "Episode 0 finished for agent cat with reward -57037.480308332815, -24.413111231467404, steps 435\n",
      "Episode 0 finished for agent toy with reward 8148.497563519094, 38.01315561749642, steps 151\n",
      "Episode 0 finished for agent cat with reward -8001.04077135809, -38.01315561749642, steps 151\n",
      "Episode 0 finished for agent toy with reward 12842.362983348126, 23.853720883753127, steps 175\n",
      "Episode 0 finished for agent cat with reward -12794.002478193144, -23.853720883753127, steps 175\n",
      "Episode 0 finished for agent toy with reward 34670.470438127326, 26.076809620810597, steps 327\n",
      "Episode 0 finished for agent cat with reward -34576.46533506449, -26.076809620810597, steps 327\n",
      "Episode 0 finished for agent toy with reward 50327.09498480094, 36.71511950137164, steps 401\n",
      "Episode 0 finished for agent cat with reward -50238.54875487497, -36.71511950137164, steps 401\n",
      "Episode 0 finished for agent toy with reward 26053.130953428656, 21.18962010041709, steps 269\n",
      "Episode 0 finished for agent cat with reward -25994.939563085856, -21.18962010041709, steps 269\n",
      "Episode 0 finished for agent toy with reward 8321.608997619362, 28.319604517012593, steps 151\n",
      "Episode 0 finished for agent cat with reward -8211.775738213473, -28.319604517012593, steps 151\n",
      "Episode 0 finished for agent cat with reward -19460.906253811005, -29.732137494637012, steps 240\n",
      "Episode 0 finished for agent toy with reward 19299.631047678187, 29.732137494637012, steps 240\n",
      "Episode 0 finished for agent toy with reward 21180.54710221235, 29.732137494637012, steps 277\n",
      "Episode 0 finished for agent cat with reward -20970.95081941452, -29.732137494637012, steps 277\n",
      "Iteration 20: cat: -28737.01, toy: 28815.14 567.4\n",
      "Episode 0 finished for agent toy with reward 116733.16529995638, 24.413111231467404, steps 645\n",
      "Episode 0 finished for agent cat with reward -116557.38130343342, -24.413111231467404, steps 645\n",
      "Episode 0 finished for agent toy with reward 33498.303060304264, 28.319604517012593, steps 403\n",
      "Episode 0 finished for agent cat with reward -33249.449919605446, -28.319604517012593, steps 403\n",
      "Episode 0 finished for agent toy with reward 30845.194884294568, 26.248809496813376, steps 331\n",
      "Episode 0 finished for agent cat with reward -30677.86203760355, -26.248809496813376, steps 331\n",
      "Episode 0 finished for agent toy with reward 6401.736742281313, 22.0, steps 131\n",
      "Episode 0 finished for agent cat with reward -6311.3726419548875, -22.0, steps 131\n",
      "Episode 0 finished for agent toy with reward 26445.00769657719, 27.018512172212592, steps 285\n",
      "Episode 0 finished for agent cat with reward -26363.995357217933, -27.018512172212592, steps 285\n",
      "Episode 0 finished for agent toy with reward 4233.909188855169, 38.27531841800928, steps 89\n",
      "Episode 0 finished for agent cat with reward -4115.688226495852, -38.27531841800928, steps 89\n",
      "Episode 0 finished for agent toy with reward 27318.418571793947, 27.65863337187866, steps 309\n",
      "Episode 0 finished for agent cat with reward -27196.715443475598, -27.65863337187866, steps 309\n",
      "Episode 0 finished for agent cat with reward -4395.490907798596, -34.48187929913333, steps 102\n",
      "Episode 0 finished for agent toy with reward 4276.272044868656, 34.48187929913333, steps 102\n",
      "Episode 0 finished for agent toy with reward 22892.8935249549, 35.35533905932738, steps 253\n",
      "Episode 0 finished for agent cat with reward -22802.96104934752, -35.35533905932738, steps 253\n",
      "Episode 0 finished for agent toy with reward 27467.525007868964, 26.076809620810597, steps 271\n",
      "Episode 0 finished for agent cat with reward -27395.423396345373, -26.076809620810597, steps 271\n",
      "Iteration 30: cat: -29906.63, toy: 30011.24 563.8\n",
      "Episode 0 finished for agent toy with reward 35083.182491194326, 23.40939982143925, steps 349\n",
      "Episode 0 finished for agent cat with reward -34910.9808661258, -23.40939982143925, steps 349\n",
      "Episode 0 finished for agent toy with reward 10367.263110410695, 30.4138126514911, steps 209\n",
      "Episode 0 finished for agent cat with reward -10193.63422779867, -30.4138126514911, steps 209\n",
      "Episode 0 finished for agent toy with reward 126802.76316725247, 21.18962010041709, steps 713\n",
      "Episode 0 finished for agent cat with reward -126549.80583257336, -21.18962010041709, steps 713\n",
      "Episode 0 finished for agent toy with reward 27437.634540296065, 22.360679774997898, steps 259\n",
      "Episode 0 finished for agent cat with reward -27339.971745355524, -22.360679774997898, steps 259\n",
      "Episode 0 finished for agent cat with reward -78205.36008356366, -29.732137494637012, steps 606\n",
      "Episode 0 finished for agent toy with reward 78276.45043783555, 29.732137494637012, steps 606\n",
      "Episode 0 finished for agent toy with reward 26309.798982795954, 31.622776601683793, steps 305\n",
      "Episode 0 finished for agent cat with reward -26164.87480316382, -31.622776601683793, steps 305\n",
      "Episode 0 finished for agent toy with reward 9897.550513489163, 23.853720883753127, steps 157\n",
      "Episode 0 finished for agent cat with reward -9813.006042727993, -23.853720883753127, steps 157\n",
      "Episode 0 finished for agent toy with reward 50543.61496512562, 26.627053911388696, steps 411\n",
      "Episode 0 finished for agent cat with reward -50405.663410856185, -26.627053911388696, steps 411\n",
      "Episode 0 finished for agent toy with reward 75153.0444313449, 29.0, steps 477\n",
      "Episode 0 finished for agent cat with reward -74993.16727432229, -29.0, steps 477\n",
      "Episode 0 finished for agent toy with reward 82749.75769269839, 27.65863337187866, steps 613\n",
      "Episode 0 finished for agent cat with reward -82547.09286096, -27.65863337187866, steps 613\n",
      "Iteration 40: cat: -52112.36, toy: 52262.11 819.8\n",
      "Episode 0 finished for agent toy with reward 38003.84764839199, 43.18564576337837, steps 335\n",
      "Episode 0 finished for agent cat with reward -37922.34142142006, -43.18564576337837, steps 335\n",
      "Episode 0 finished for agent toy with reward 102820.62056541897, 20.615528128088304, steps 597\n",
      "Episode 0 finished for agent cat with reward -102709.59511240406, -20.615528128088304, steps 597\n",
      "Episode 0 finished for agent toy with reward 52563.544110041556, 21.095023109728988, steps 357\n",
      "Episode 0 finished for agent cat with reward -52488.1309256055, -21.095023109728988, steps 357\n",
      "Episode 0 finished for agent toy with reward 2786.616394925606, 34.539832078341085, steps 67\n",
      "Episode 0 finished for agent cat with reward -2694.7162305098345, -34.539832078341085, steps 67\n",
      "Episode 0 finished for agent toy with reward 11007.833532749095, 20.8806130178211, steps 165\n",
      "Episode 0 finished for agent cat with reward -10938.777130666032, -20.8806130178211, steps 165\n",
      "Episode 0 finished for agent toy with reward 10086.700232412217, 35.22782990761707, steps 187\n",
      "Episode 0 finished for agent cat with reward -9939.149072683002, -35.22782990761707, steps 187\n",
      "Episode 0 finished for agent cat with reward -34080.37408325168, -26.248809496813376, steps 360\n",
      "Episode 0 finished for agent toy with reward 34035.18641583862, 26.248809496813376, steps 360\n",
      "Episode 0 finished for agent toy with reward 0.0, 0.0, steps 1\n",
      "Episode 0 finished for agent cat with reward 100.0, 0.0, steps 1\n",
      "Episode 0 finished for agent toy with reward 118228.99854322302, 23.853720883753127, steps 729\n",
      "Episode 0 finished for agent cat with reward -118018.75247734136, -23.853720883753127, steps 729\n",
      "Episode 0 finished for agent toy with reward 13145.137644688764, 31.04834939252005, steps 199\n",
      "Episode 0 finished for agent cat with reward -13031.489104466362, -31.04834939252005, steps 199\n",
      "Iteration 50: cat: -38172.33, toy: 38267.85 599.4\n",
      "Episode 0 finished for agent toy with reward 13620.199694818975, 23.08679276123039, steps 251\n",
      "Episode 0 finished for agent cat with reward -13424.306983729064, -23.08679276123039, steps 251\n",
      "Episode 0 finished for agent toy with reward 10046.884926998928, 27.018512172212592, steps 187\n",
      "Episode 0 finished for agent cat with reward -9864.244259303714, -27.018512172212592, steps 187\n",
      "Episode 0 finished for agent toy with reward 25042.61964002595, 42.44997055358225, steps 309\n",
      "Episode 0 finished for agent cat with reward -24892.78576973906, -42.44997055358225, steps 309\n",
      "Episode 0 finished for agent toy with reward 102341.54459118895, 38.01315561749642, steps 687\n",
      "Episode 0 finished for agent cat with reward -102051.41456282993, -38.01315561749642, steps 687\n",
      "Episode 0 finished for agent toy with reward 9540.152711285522, 34.88552708502482, steps 189\n",
      "Episode 0 finished for agent cat with reward -9381.59416557655, -34.88552708502482, steps 189\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 学習\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dqn(agent_dict, env_learning, num_iterations, num_episodes_per_iteration)\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(agent_dict, env, num_iterations, num_episodes_per_iteration)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m%\u001b[39m replay_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m replay_agent \u001b[38;5;129;01min\u001b[39;00m agent_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 30\u001b[0m             agent_dict[replay_agent]\u001b[38;5;241m.\u001b[39mreplay()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     33\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# No action needed if agent is done\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 83\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     82\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# εを減少させる\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    237\u001b[0m         group,\n\u001b[1;32m    238\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         state_steps,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     adam(\n\u001b[1;32m    247\u001b[0m         params_with_grad,\n\u001b[1;32m    248\u001b[0m         grads,\n\u001b[1;32m    249\u001b[0m         exp_avgs,\n\u001b[1;32m    250\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    251\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    252\u001b[0m         state_steps,\n\u001b[1;32m    253\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    254\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    255\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    256\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    257\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    258\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    259\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    260\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    261\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    262\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    263\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    264\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    265\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    266\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    267\u001b[0m         decoupled_weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoupled_weight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 933\u001b[0m func(\n\u001b[1;32m    934\u001b[0m     params,\n\u001b[1;32m    935\u001b[0m     grads,\n\u001b[1;32m    936\u001b[0m     exp_avgs,\n\u001b[1;32m    937\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    938\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    939\u001b[0m     state_steps,\n\u001b[1;32m    940\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    941\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    942\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    943\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    944\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    945\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    946\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    947\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    948\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    949\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    950\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    951\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    952\u001b[0m     decoupled_weight_decay\u001b[38;5;241m=\u001b[39mdecoupled_weight_decay,\n\u001b[1;32m    953\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:416\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m             grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m--> 416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[1;32m    417\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n\u001b[1;32m    418\u001b[0m     exp_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "train_dqn(agent_dict, env_learning, num_iterations, num_episodes_per_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . C . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . T . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "------------------------------------------------------------\n",
      "agent: cat, count: 130, cat: 104, 269, toy: 93, 250\n",
      "mean_reward: (-323782.79339811375, 0.0) +/- (321860.9502775147, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# 評価用環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_eval = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデル評価\n",
    "mean_std_rewards = evaluate_model(agent_dict, env_eval, n_eval_episodes=1)\n",
    "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "save_dqn(agent_dict, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . C . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". T . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "------------------------------------------------------------\n",
      "agent: toy, count: 957, cat: 426, 513, toy: 38, 151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m load_dqn(env_eval, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoy\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ロードしたモデルの評価\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m mean_std_rewards \u001b[38;5;241m=\u001b[39m evaluate_model(loaded_model, env_eval, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_std_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_std_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 74\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(agent_dict, eval_env, n_eval_episodes)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m         action \u001b[38;5;241m=\u001b[39m agent_dict[agent]\u001b[38;5;241m.\u001b[39mact(obs)  \u001b[38;5;66;03m# 各エージェントに行動させる\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     75\u001b[0m     episode_rewards[agent] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# 各agentごとに報酬を記録\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_name \u001b[38;5;129;01min\u001b[39;00m reward_sums:\n",
      "File \u001b[0;32m~/workspace/cat-brain/cat-dqn/cat_toy_env.py:119\u001b[0m, in \u001b[0;36mCatToyEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_selector\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/workspace/cat-brain/cat-dqn/cat_toy_env.py:152\u001b[0m, in \u001b[0;36mCatToyEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_x\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_y\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, toy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoy_x\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoy_y\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# フレーム間の遅延（1フレームごとの更新時間）\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.05\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 評価用環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_eval = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "loaded_model = load_dqn(env_eval, [\"cat\", \"toy\"], \"models\")\n",
    "\n",
    "# ロードしたモデルの評価\n",
    "mean_std_rewards = evaluate_model(loaded_model, env_eval, n_eval_episodes=10)\n",
    "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力の2つのTensorを結合\n",
    "toy = torch.randn(1, 2)\n",
    "cat = torch.randn(1, 2)\n",
    "concat_input = torch.cat([toy, cat], dim=1)  # shape: (1, 4)\n",
    "\n",
    "# エクスポート対象モデル（例: policyネットワーク）\n",
    "# dummyの環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_dummy = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "loaded_model = load_dqn(env_dummy, [\"cat\", \"toy\"], \"models\")\n",
    "policy_net = loaded_model[\"cat\"].model  # catエージェントのポリシーネットワークを取得\n",
    "\n",
    "# ONNXエクスポート\n",
    "torch.onnx.export(\n",
    "    policy_net,\n",
    "    concat_input,  # ← dictではなく単一Tensor\n",
    "    \"cat_dqn_policy.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    input_names=[\"obs\"],\n",
    "    output_names=[\"q_values\"],\n",
    "    dynamic_axes={\n",
    "        \"obs\": {0: \"batch_size\"},\n",
    "        \"q_values\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境のクローズ\n",
    "env_learning.close()\n",
    "env_eval.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習済みモデルの使用 (play.py)\n",
    "\n",
    "このセルでは、学習済みのモデルをロードし、`CartPole-v1`環境でエージェントがどのように行動するかを観察します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "import time\n",
    "from cat_toy_env import CatToyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs=dict(render_mode=\"\", max_steps=1000, cat_speed = 2)\n",
    "\n",
    "# 環境の作成\n",
    "env = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "model_playing = DQN.load(\"cat_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エピソードの実行\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model_playing.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(\"観測:\", obs)\n",
    "    done = terminated or truncated\n",
    "    env.render()  # 環境の描画\n",
    "    #time.sleep(0.001) # 0.01秒待機"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境のクローズ\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
