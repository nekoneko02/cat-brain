{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium stable-baselines3 torch\n",
    "%pip install stable-baselines3[extra]\n",
    "%pip install sb3-contrib\n",
    "%pip install torch torchvision\n",
    "%pip install numpy protobuf onnx onnxruntime\n",
    "%pip install onnx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習モデルの学習 (main.py)\n",
    "\n",
    "このセルでは、DQNアルゴリズムを用いて、`CartPole-v1`環境でモデルを学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import api_test\n",
    "from cat_toy_env import CatToyEnv\n",
    "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
    "\n",
    "# 1個だけ環境を作る（並列ではなく）\n",
    "env = CatToyEnv(**env_kwargs)\n",
    "api_test(env, num_cycles=1000, verbose_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from cat_toy_env import CatToyEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1個だけ環境を作る（並列ではなく）\n",
    "env_preview = CatToyEnv(**env_kwargs)\n",
    "\n",
    "obs = env_preview.reset()\n",
    "\n",
    "# 観測のshapeを確認\n",
    "print(\"観測の形:\", obs)\n",
    "print(\"観測の中身:\", obs)\n",
    "# 学習用環境\n",
    "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
    "env_learning = CatToyEnv(**env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# 定義：ニューラルネットワーク（Qネットワーク）\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "class DQNAgent:\n",
    "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.agent_name = agent_name  # エージェント名（'cat' または 'toy'）\n",
    "        self.action_space = env.action_spaces[self.agent_name]  # 各エージェントに対応するアクション空間\n",
    "        self.state_space = env.observation_spaces[self.agent_name].shape[0]  # 各エージェントに対応する観察空間\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = DQN(self.state_space, self.action_space.n)  # 各エージェント用のDQNモデル\n",
    "        self.target_model = DQN(self.state_space, self.action_space.n)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 64\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return self.action_space.sample()  # ランダム行動\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # バッチ次元を追加\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()  # 最大Q値に基づいて行動を選択\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Q値の計算\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "        # 損失計算とバックプロパゲーション\n",
    "        loss = self.loss_fn(current_q_values.squeeze(1), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # εを減少させる\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        torch.save(self.model.state_dict(), filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        self.model.load_state_dict(torch.load(filepath))\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.utils import agent_selector\n",
    "\n",
    "def train_dqn(agent_dict, env, num_episodes=10000):\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        total_rewards = {agent: 0.0 for agent in env.agents}\n",
    "        _done = False\n",
    "        for agent in env.agent_iter():\n",
    "            if agent not in env.agents:\n",
    "                break  # 全エージェントが終了していたらループを抜ける\n",
    "            obs, reward, terminated, truncated, info = env.last()\n",
    "            done = terminated or truncated\n",
    "            total_rewards[agent] += reward\n",
    "\n",
    "            if done:\n",
    "                action = None  # No action needed if agent is done\n",
    "            else:\n",
    "                action = agent_dict[agent].act(obs)\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "            if not done:\n",
    "                # 次の状態の取得\n",
    "                next_obs, next_reward, next_done, _, _ = env.last(observe=True)\n",
    "\n",
    "                agent_dict[agent].store_experience(obs, action, reward, next_obs, float(done))\n",
    "                agent_dict[agent].replay()\n",
    "\n",
    "        # ログ出力\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}: \" + \", \".join([f\"{a}: {r:.2f}\" for a, r in total_rewards.items()]))\n",
    "\n",
    "        # ターゲットネットワーク更新\n",
    "        if episode % 1000 == 0:\n",
    "            for agent in agent_dict.values():\n",
    "                agent.update_target_model()\n",
    "\n",
    "def evaluate_model(agent_dict, eval_env, n_eval_episodes=10):\n",
    "    reward_sums = {agent_name: [] for agent_name in agent_dict.keys()}\n",
    "\n",
    "    for _ in range(n_eval_episodes):\n",
    "        env = eval_env  # 環境がreset可能で、内部状態が共有でないと仮定\n",
    "        env.reset()\n",
    "        episode_rewards = {agent_name: 0.0 for agent_name in agent_dict.keys()}\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            done = termination or truncation\n",
    "\n",
    "            if done:\n",
    "                action = None  # 終了したら行動不要\n",
    "            else:\n",
    "                action = agent_dict[agent].act(obs)  # 各エージェントに行動させる\n",
    "\n",
    "            env.step(action)\n",
    "            episode_rewards[agent] += reward  # 各agentごとに報酬を記録\n",
    "\n",
    "        for agent_name in reward_sums:\n",
    "            reward_sums[agent_name].append(episode_rewards[agent_name])\n",
    "\n",
    "    # 統計量（平均・標準偏差）を返す\n",
    "    mean_std_rewards = {\n",
    "        agent: (np.mean(rewards), np.std(rewards))\n",
    "        for agent, rewards in reward_sums.items()\n",
    "    }\n",
    "\n",
    "    return mean_std_rewards\n",
    "\n",
    "def save_dqn(agent_dict, base_path = \"models\"):\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    for agent_name, agent in agent_dict.items():\n",
    "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
    "        agent.save_model(filepath)\n",
    "\n",
    "def load_dqn(env, agents = [\"cat\", \"toy\"] , base_path = \"models\"):\n",
    "    agent_dict = {}\n",
    "    for agent_name in agents:\n",
    "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
    "        agent = DQNAgent(agent_name, env)\n",
    "        agent.load_model(filepath)\n",
    "        agent_dict[agent_name] = agent\n",
    "    return agent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントの作成\n",
    "agent_dict = {\n",
    "    agent_name: DQNAgent(agent_name, env_learning)\n",
    "    for agent_name in env_learning.agents\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "train_dqn(agent_dict, env_learning, num_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル評価\n",
    "mean_std_rewards = evaluate_model(agent_dict, env_eval, n_eval_episodes=10)\n",
    "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "save_dqn(agent_dict, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_eval = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "loaded_model = load_dqn(env_eval, [\"cat\", \"toy\"], \"models\")\n",
    "\n",
    "# ロードしたモデルの評価\n",
    "mean_std_rewards = evaluate_model(loaded_model, env_eval, n_eval_episodes=10)\n",
    "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力の2つのTensorを結合\n",
    "toy = torch.randn(1, 2)\n",
    "cat = torch.randn(1, 2)\n",
    "concat_input = torch.cat([toy, cat], dim=1)  # shape: (1, 4)\n",
    "\n",
    "# エクスポート対象モデル（例: policyネットワーク）\n",
    "policy_net = loaded_model.policy.q_net\n",
    "\n",
    "# ONNXエクスポート\n",
    "torch.onnx.export(\n",
    "    policy_net,\n",
    "    concat_input,  # ← dictではなく単一Tensor\n",
    "    \"cat_dqn_policy.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    input_names=[\"obs\"],\n",
    "    output_names=[\"q_values\"],\n",
    "    dynamic_axes={\n",
    "        \"obs\": {0: \"batch_size\"},\n",
    "        \"q_values\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境のクローズ\n",
    "env_learning.close()\n",
    "env_eval.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習済みモデルの使用 (play.py)\n",
    "\n",
    "このセルでは、学習済みのモデルをロードし、`CartPole-v1`環境でエージェントがどのように行動するかを観察します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "import time\n",
    "from cat_toy_env import CatToyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs=dict(render_mode=\"\", max_steps=1000, cat_speed = 2)\n",
    "\n",
    "# 環境の作成\n",
    "env = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "model_playing = DQN.load(\"cat_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エピソードの実行\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model_playing.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(\"観測:\", obs)\n",
    "    done = terminated or truncated\n",
    "    env.render()  # 環境の描画\n",
    "    #time.sleep(0.001) # 0.01秒待機"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境のクローズ\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
