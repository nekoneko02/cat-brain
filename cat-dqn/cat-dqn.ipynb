{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium stable-baselines3 torch\n",
    "%pip install stable-baselines3[extra]\n",
    "%pip install sb3-contrib\n",
    "%pip install torch torchvision\n",
    "%pip install numpy protobuf onnx onnxruntime\n",
    "%pip install onnx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習モデルの学習 (main.py)\n",
    "\n",
    "このセルでは、DQNアルゴリズムを用いて、`CartPole-v1`環境でモデルを学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import api_test\n",
    "from cat_toy_env import CatToyEnv\n",
    "env_kwargs=dict(render_mode=None, max_steps=1000)\n",
    "\n",
    "# 1個だけ環境を作る（並列ではなく）\n",
    "env = CatToyEnv(**env_kwargs)\n",
    "api_test(env, num_cycles=1000, verbose_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from cat_toy_env import CatToyEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100\n",
    "num_episodes_per_iteration = 1\n",
    "num_steps_per_episode = 1000\n",
    "# num_epoches = 1\n",
    "# num_replays_per_episode = num_epoches * num_episodes_per_iteration * num_steps_per_episode\n",
    "update_target_steps = 10\n",
    "replay_interval = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "観測の形: [753. 193. 526.  83.]\n",
      "観測の中身: [753. 193. 526.  83.]\n"
     ]
    }
   ],
   "source": [
    "env_kwargs=dict(render_mode=None, max_steps = num_steps_per_episode)\n",
    "# 1個だけ環境を作る\n",
    "env_preview = CatToyEnv(**env_kwargs)\n",
    "\n",
    "obs = env_preview.reset()\n",
    "\n",
    "# 観測のshapeを確認\n",
    "print(\"観測の形:\", obs)\n",
    "print(\"観測の中身:\", obs)\n",
    "# 学習用環境\n",
    "env_learning = CatToyEnv(**env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# 定義：ニューラルネットワーク（Qネットワーク）\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "class DQNAgent:\n",
    "    def __init__(self, agent_name, env, learning_rate=1e-4, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.agent_name = agent_name  # エージェント名（'cat' または 'toy'）\n",
    "        self.action_space = env.action_spaces[self.agent_name]  # 各エージェントに対応するアクション空間\n",
    "        self.state_space = env.observation_spaces[self.agent_name].shape[0]  # 各エージェントに対応する観察空間\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = DQN(self.state_space, self.action_space.n)  # 各エージェント用のDQNモデル\n",
    "        self.target_model = DQN(self.state_space, self.action_space.n)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 64\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((\n",
    "            np.array(state, dtype=np.float32),\n",
    "            action,\n",
    "            reward,\n",
    "            np.array(next_state, dtype=np.float32),\n",
    "            done\n",
    "        ))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return self.action_space.sample()  # ランダム行動\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # バッチ次元を追加\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()  # 最大Q値に基づいて行動を選択\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Q値の計算\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "        # 損失計算とバックプロパゲーション\n",
    "        loss = self.loss_fn(current_q_values.squeeze(1), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # εを減少させる\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        torch.save(self.model.state_dict(), filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        self.model.load_state_dict(torch.load(filepath))\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent_dict, env, num_iterations, num_episodes_per_iteration):\n",
    "    total_rewards = {agent: 0.0 for agent in env.agents}\n",
    "    rewards = {agent: 0.0 for agent in env.agents}\n",
    "    steps = 0\n",
    "    for iteration in range(num_iterations):\n",
    "        for episode in range(num_episodes_per_iteration):\n",
    "            obs = env.reset()\n",
    "            prev_obs = {agent: obs for agent in env.agents}\n",
    "            prev_action = {agent: None for agent in env.agents}\n",
    "\n",
    "            for agent in env.agent_iter():\n",
    "                obs, total_reward, terminated, truncated, _ = env.last()\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if not done:\n",
    "                    rewards = env.rewards.copy()  # 各エージェントの報酬を更新\n",
    "\n",
    "                if prev_action[agent] is not None:\n",
    "                    # 前回行動の結果が今回のループで得られたので、ここで保存できる\n",
    "                    agent_dict[agent].store_experience(\n",
    "                        prev_obs[agent],         # s\n",
    "                        prev_action[agent],      # a\n",
    "                        rewards[agent],      # r (現在のループで得られた報酬)\n",
    "                        obs,                     # s' (次状態)\n",
    "                        float(done)              # done\n",
    "                    )\n",
    "                    # ここでreplayを行う\n",
    "                    if env.step_count % replay_interval == 0:\n",
    "                        for replay_agent in agent_dict.keys():\n",
    "                            agent_dict[replay_agent].replay()\n",
    "\n",
    "                if done:\n",
    "                    action = None  # No action needed if agent is done\n",
    "                    total_rewards[agent] += total_reward\n",
    "                    steps += env.step_count\n",
    "                    print(f\"Episode {episode} finished for agent {agent} with reward {total_reward}, {rewards[agent]}, steps {env.step_count}\")\n",
    "                else:\n",
    "                    action = agent_dict[agent].act(obs)\n",
    "\n",
    "                env.step(action)\n",
    "\n",
    "                prev_obs[agent] = obs  # 次の状態を更新\n",
    "                prev_action[agent] = action  # 次の行動を更新\n",
    "\n",
    "        # ログ出力\n",
    "        if iteration % update_target_steps == 0:\n",
    "            print(f\"Iteration {iteration}: \" + \", \".join([f\"{a}: {r / update_target_steps:.2f}\" for a, r in total_rewards.items()]), steps / update_target_steps)\n",
    "            total_rewards = {agent: 0.0 for agent in total_rewards.keys()}\n",
    "            steps = 0\n",
    "\n",
    "        # ターゲットネットワーク更新\n",
    "        if iteration % update_target_steps == 0:\n",
    "            for agent in agent_dict.values():\n",
    "                agent.update_target_model()\n",
    "\n",
    "def evaluate_model(agent_dict, eval_env, n_eval_episodes=10):\n",
    "    reward_sums = {agent_name: [] for agent_name in agent_dict.keys()}\n",
    "\n",
    "    for _ in range(n_eval_episodes):\n",
    "        env = eval_env  # 環境がreset可能で、内部状態が共有でないと仮定\n",
    "        env.reset()\n",
    "        episode_rewards = {agent_name: 0.0 for agent_name in agent_dict.keys()}\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            done = termination or truncation\n",
    "\n",
    "            if done:\n",
    "                action = None  # 終了したら行動不要\n",
    "            else:\n",
    "                action = agent_dict[agent].act(obs)  # 各エージェントに行動させる\n",
    "\n",
    "            env.step(action)\n",
    "            episode_rewards[agent] += reward  # 各agentごとに報酬を記録\n",
    "\n",
    "        for agent_name in reward_sums:\n",
    "            reward_sums[agent_name].append(episode_rewards[agent_name])\n",
    "\n",
    "    # 統計量（平均・標準偏差）を返す\n",
    "    mean_std_rewards = {\n",
    "        agent: (np.mean(rewards), np.std(rewards))\n",
    "        for agent, rewards in reward_sums.items()\n",
    "    }\n",
    "\n",
    "    return mean_std_rewards\n",
    "\n",
    "def save_dqn(agent_dict, base_path = \"models\"):\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    for agent_name, agent in agent_dict.items():\n",
    "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
    "        agent.save_model(filepath)\n",
    "\n",
    "def load_dqn(env, agents = [\"cat\", \"toy\"] , base_path = \"models\"):\n",
    "    agent_dict = {}\n",
    "    for agent_name in agents:\n",
    "        filepath = os.path.join(base_path, f\"{agent_name}_model.pth\")\n",
    "        agent = DQNAgent(agent_name, env)\n",
    "        agent.load_model(filepath)\n",
    "        agent_dict[agent_name] = agent\n",
    "    return agent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントの作成\n",
    "agent_dict = {\n",
    "    agent_name: DQNAgent(agent_name, env_learning)\n",
    "    for agent_name in env_learning.agents\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished for agent toy with reward 209972.70667458168, 117.13667231059623, steps 1001\n",
      "Episode 0 finished for agent cat with reward -210107.3150822329, -117.13667231059623, steps 1001\n",
      "Iteration 0: cat: -21010.73, toy: 20997.27 200.2\n",
      "Episode 0 finished for agent toy with reward 234025.50133725372, 521.7250233600072, steps 1001\n",
      "Episode 0 finished for agent cat with reward -234358.93683309486, -521.7250233600072, steps 1001\n",
      "Episode 0 finished for agent toy with reward 165678.74240916732, 231.0, steps 1001\n",
      "Episode 0 finished for agent cat with reward -165891.2688880913, -231.0, steps 1001\n",
      "Episode 0 finished for agent toy with reward 289511.1453274985, 795.0654061144907, steps 1001\n",
      "Episode 0 finished for agent cat with reward -290099.740335931, -795.0654061144907, steps 1001\n",
      "Episode 0 finished for agent toy with reward 363969.66183928587, 694.4595020589753, steps 1001\n",
      "Episode 0 finished for agent cat with reward -364553.2504779781, -694.4595020589753, steps 1001\n",
      "Episode 0 finished for agent toy with reward 335150.7117217512, 665.0902194439489, steps 1001\n",
      "Episode 0 finished for agent cat with reward -335765.3949469965, -665.0902194439489, steps 1001\n",
      "Episode 0 finished for agent toy with reward 279587.65472403326, 643.6210064937284, steps 1001\n",
      "Episode 0 finished for agent cat with reward -280019.55493762885, -643.6210064937284, steps 1001\n",
      "Episode 0 finished for agent toy with reward 264911.04135227075, 591.9974662107938, steps 1001\n",
      "Episode 0 finished for agent cat with reward -265345.4445762663, -591.9974662107938, steps 1001\n",
      "Episode 0 finished for agent toy with reward 205511.26706680396, 459.03921401117793, steps 1001\n",
      "Episode 0 finished for agent cat with reward -205936.02555928903, -459.03921401117793, steps 1001\n",
      "Episode 0 finished for agent toy with reward 132822.83604594326, 279.0, steps 1001\n",
      "Episode 0 finished for agent cat with reward -132965.31843713112, -279.0, steps 1001\n",
      "Episode 0 finished for agent toy with reward 301941.7070911816, 632.1787721839448, steps 1001\n",
      "Episode 0 finished for agent cat with reward -302496.1106025379, -632.1787721839448, steps 1001\n",
      "Iteration 10: cat: -257743.10, toy: 257311.03 2002.0\n",
      "Episode 0 finished for agent toy with reward 196949.30752861054, 343.14137028344453, steps 1001\n",
      "Episode 0 finished for agent cat with reward -197124.1041942962, -343.14137028344453, steps 1001\n",
      "Episode 0 finished for agent toy with reward 274602.2275293482, 646.0030959678135, steps 1001\n",
      "Episode 0 finished for agent cat with reward -274999.73973801185, -646.0030959678135, steps 1001\n",
      "Episode 0 finished for agent toy with reward 281010.576408627, 640.9321336927959, steps 1001\n",
      "Episode 0 finished for agent cat with reward -281586.46247625584, -640.9321336927959, steps 1001\n",
      "Episode 0 finished for agent toy with reward 305416.8008763026, 583.1989368988939, steps 1001\n",
      "Episode 0 finished for agent cat with reward -305938.8366356304, -583.1989368988939, steps 1001\n",
      "Episode 0 finished for agent toy with reward 164177.0541782483, 133.09395177843356, steps 1001\n",
      "Episode 0 finished for agent cat with reward -164082.62461986454, -133.09395177843356, steps 1001\n",
      "Episode 0 finished for agent toy with reward 383573.958024284, 679.7793759742935, steps 1001\n",
      "Episode 0 finished for agent cat with reward -384233.9906861794, -679.7793759742935, steps 1001\n",
      "Episode 0 finished for agent toy with reward 209455.4227373161, 574.8912940721924, steps 1001\n",
      "Episode 0 finished for agent cat with reward -209672.35509877634, -574.8912940721924, steps 1001\n",
      "Episode 0 finished for agent toy with reward 0.0, 0.0, steps 1\n",
      "Episode 0 finished for agent cat with reward 100.0, 0.0, steps 1\n",
      "Episode 0 finished for agent toy with reward 216058.52571181173, 410.440007796511, steps 1001\n",
      "Episode 0 finished for agent cat with reward -216554.33179037925, -410.440007796511, steps 1001\n",
      "Episode 0 finished for agent toy with reward 171755.46401167946, 397.3638131486057, steps 1001\n",
      "Episode 0 finished for agent cat with reward -172051.58273589358, -397.3638131486057, steps 1001\n",
      "Iteration 20: cat: -220614.40, toy: 220299.93 1802.0\n",
      "Episode 0 finished for agent toy with reward 296601.60994549363, 634.1048809148215, steps 1001\n",
      "Episode 0 finished for agent cat with reward -296993.88315904647, -634.1048809148215, steps 1001\n",
      "Episode 0 finished for agent toy with reward 214969.23538023932, 502.09959171463186, steps 1001\n",
      "Episode 0 finished for agent cat with reward -215334.79908642027, -502.09959171463186, steps 1001\n",
      "Episode 0 finished for agent toy with reward 196639.07580127657, 407.0, steps 1001\n",
      "Episode 0 finished for agent cat with reward -197085.14103884704, -407.0, steps 1001\n",
      "Episode 0 finished for agent toy with reward 279942.94324635813, 553.7192429381519, steps 1001\n",
      "Episode 0 finished for agent cat with reward -280493.11382160685, -553.7192429381519, steps 1001\n",
      "Episode 0 finished for agent toy with reward 98235.83615614484, 234.71898091121648, steps 1001\n",
      "Episode 0 finished for agent cat with reward -98372.57081694271, -234.71898091121648, steps 1001\n",
      "Episode 0 finished for agent toy with reward 307103.9727366696, 618.0113267570425, steps 1001\n",
      "Episode 0 finished for agent cat with reward -307701.10005273134, -618.0113267570425, steps 1001\n",
      "Episode 0 finished for agent toy with reward 202173.4873466638, 485.2720886265766, steps 1001\n",
      "Episode 0 finished for agent cat with reward -202320.88312385522, -485.2720886265766, steps 1001\n",
      "Episode 0 finished for agent toy with reward 146767.99393969116, 266.73207531153804, steps 1001\n",
      "Episode 0 finished for agent cat with reward -146932.31624266415, -266.73207531153804, steps 1001\n",
      "Episode 0 finished for agent toy with reward 172188.77380064447, 348.037354317033, steps 1001\n",
      "Episode 0 finished for agent cat with reward -172478.21220356866, -348.037354317033, steps 1001\n",
      "Episode 0 finished for agent toy with reward 207930.77941106661, 381.52457325839447, steps 1001\n",
      "Episode 0 finished for agent cat with reward -208159.3685588413, -381.52457325839447, steps 1001\n",
      "Iteration 30: cat: -212587.14, toy: 212255.37 2002.0\n",
      "Episode 0 finished for agent cat with reward -129948.59994590856, -27.586228448267445, steps 706\n",
      "Episode 0 finished for agent toy with reward 129868.38507707023, 27.586228448267445, steps 706\n",
      "Episode 0 finished for agent toy with reward 174404.4733785949, 394.6808837529378, steps 1001\n",
      "Episode 0 finished for agent cat with reward -174672.93736392958, -394.6808837529378, steps 1001\n",
      "Episode 0 finished for agent toy with reward 124374.03854222462, 323.48261158832014, steps 1001\n",
      "Episode 0 finished for agent cat with reward -124594.38030084567, -323.48261158832014, steps 1001\n",
      "Episode 0 finished for agent toy with reward 94872.65486415669, 165.94577427581578, steps 1001\n",
      "Episode 0 finished for agent cat with reward -95048.0985331429, -165.94577427581578, steps 1001\n",
      "Episode 0 finished for agent toy with reward 132409.6440080216, 429.7731959999367, steps 1001\n",
      "Episode 0 finished for agent cat with reward -132707.21032922596, -429.7731959999367, steps 1001\n",
      "Episode 0 finished for agent toy with reward 41691.18979459618, 20.0, steps 429\n",
      "Episode 0 finished for agent cat with reward -41574.04935225249, -20.0, steps 429\n",
      "Episode 0 finished for agent toy with reward 186869.68036331338, 197.00253805471644, steps 1001\n",
      "Episode 0 finished for agent cat with reward -186941.81327005097, -197.00253805471644, steps 1001\n",
      "Episode 0 finished for agent toy with reward 127414.42169755121, 210.58252539087852, steps 1001\n",
      "Episode 0 finished for agent cat with reward -127640.96402309134, -210.58252539087852, steps 1001\n",
      "Episode 0 finished for agent toy with reward 197843.009515788, 291.8030157486382, steps 1001\n",
      "Episode 0 finished for agent cat with reward -198075.25893849542, -291.8030157486382, steps 1001\n",
      "Episode 0 finished for agent toy with reward 137447.64828550743, 410.396150079408, steps 1001\n",
      "Episode 0 finished for agent cat with reward -137612.0681931406, -410.396150079408, steps 1001\n",
      "Iteration 40: cat: -134881.54, toy: 134719.51 1828.6\n",
      "Episode 0 finished for agent toy with reward 117411.50965274159, 152.31546211727817, steps 1001\n",
      "Episode 0 finished for agent cat with reward -117382.02835230967, -152.31546211727817, steps 1001\n",
      "Episode 0 finished for agent toy with reward 126941.39108375131, 142.21462653327893, steps 1001\n",
      "Episode 0 finished for agent cat with reward -126953.40190655628, -142.21462653327893, steps 1001\n",
      "Episode 0 finished for agent toy with reward 113571.4900513865, 75.8023746329889, steps 1001\n",
      "Episode 0 finished for agent cat with reward -113838.51003601111, -75.8023746329889, steps 1001\n",
      "Episode 0 finished for agent toy with reward 242376.9076502253, 228.0, steps 1001\n",
      "Episode 0 finished for agent cat with reward -242348.8060595378, -228.0, steps 1001\n",
      "Episode 0 finished for agent toy with reward 153816.13000878718, 223.60679774997897, steps 1001\n",
      "Episode 0 finished for agent cat with reward -154113.49372238206, -223.60679774997897, steps 1001\n",
      "Episode 0 finished for agent toy with reward 201938.73792547933, 367.535032343857, steps 1001\n",
      "Episode 0 finished for agent cat with reward -202158.92521127634, -367.535032343857, steps 1001\n",
      "Episode 0 finished for agent toy with reward 99399.18364176861, 148.00337833982033, steps 1001\n",
      "Episode 0 finished for agent cat with reward -99557.00969766188, -148.00337833982033, steps 1001\n",
      "Episode 0 finished for agent toy with reward 163357.57453600623, 314.35807608521844, steps 1001\n",
      "Episode 0 finished for agent cat with reward -163593.1498884819, -314.35807608521844, steps 1001\n",
      "Episode 0 finished for agent toy with reward 90613.91230022251, 169.0, steps 1001\n",
      "Episode 0 finished for agent cat with reward -90647.53089736174, -169.0, steps 1001\n",
      "Episode 0 finished for agent toy with reward 94909.87859482468, 260.16341018675166, steps 1001\n",
      "Episode 0 finished for agent cat with reward -95132.82058618616, -260.16341018675166, steps 1001\n",
      "Iteration 50: cat: -140572.57, toy: 140433.67 2002.0\n",
      "Episode 0 finished for agent toy with reward 104424.48739684657, 139.40229553346674, steps 1001\n",
      "Episode 0 finished for agent cat with reward -104212.49010951041, -139.40229553346674, steps 1001\n",
      "Episode 0 finished for agent toy with reward 79238.37415435411, 152.73833834371774, steps 1001\n",
      "Episode 0 finished for agent cat with reward -79241.30418750284, -152.73833834371774, steps 1001\n",
      "Episode 0 finished for agent toy with reward 142918.9503611508, 105.38026380684383, steps 1001\n",
      "Episode 0 finished for agent cat with reward -143206.1085617384, -105.38026380684383, steps 1001\n",
      "Episode 0 finished for agent toy with reward 90566.02498875253, 76.84399781375251, steps 1001\n",
      "Episode 0 finished for agent cat with reward -90497.66045300988, -76.84399781375251, steps 1001\n",
      "Episode 0 finished for agent toy with reward 29.068883707497267, 29.068883707497267, steps 3\n",
      "Episode 0 finished for agent cat with reward 70.26786250536298, -29.068883707497267, steps 3\n",
      "Episode 0 finished for agent toy with reward 84607.03281096293, 191.7602670002313, steps 1001\n",
      "Episode 0 finished for agent cat with reward -84673.87145069972, -191.7602670002313, steps 1001\n",
      "Episode 0 finished for agent toy with reward 73939.3040155132, 90.52071586106685, steps 1001\n",
      "Episode 0 finished for agent cat with reward -73777.46016861018, -90.52071586106685, steps 1001\n",
      "Episode 0 finished for agent toy with reward 74140.46814457786, 106.07544484940895, steps 1001\n",
      "Episode 0 finished for agent cat with reward -73910.11178917506, -106.07544484940895, steps 1001\n",
      "Episode 0 finished for agent toy with reward 191111.10575659224, 350.09141663285607, steps 1001\n",
      "Episode 0 finished for agent cat with reward -191464.39129576812, -350.09141663285607, steps 1001\n",
      "Episode 0 finished for agent toy with reward 187370.68374330655, 133.9589489358587, steps 1001\n",
      "Episode 0 finished for agent cat with reward -187527.4094476122, -133.9589489358587, steps 1001\n",
      "Iteration 60: cat: -102844.05, toy: 102834.55 1802.4\n",
      "Episode 0 finished for agent toy with reward 143193.50873738521, 159.11002482559041, steps 1001\n",
      "Episode 0 finished for agent cat with reward -143254.63957088254, -159.11002482559041, steps 1001\n",
      "Episode 0 finished for agent toy with reward 109283.55953003271, 207.18349355100662, steps 1001\n",
      "Episode 0 finished for agent cat with reward -109429.5201845346, -207.18349355100662, steps 1001\n",
      "Episode 0 finished for agent toy with reward 31127.27646147763, 34.438350715445125, steps 651\n",
      "Episode 0 finished for agent cat with reward -31067.893013016946, -34.438350715445125, steps 651\n",
      "Episode 0 finished for agent toy with reward 129347.76993486795, 225.44178849538966, steps 1001\n",
      "Episode 0 finished for agent cat with reward -129422.061164192, -225.44178849538966, steps 1001\n",
      "Episode 0 finished for agent toy with reward 96169.59002450254, 195.143536915779, steps 1001\n",
      "Episode 0 finished for agent cat with reward -96260.42721060038, -195.143536915779, steps 1001\n",
      "Episode 0 finished for agent toy with reward 82230.2792795163, 165.22106403240477, steps 1001\n",
      "Episode 0 finished for agent cat with reward -82190.09953666298, -165.22106403240477, steps 1001\n",
      "Episode 0 finished for agent toy with reward 185307.25644806612, 172.2440129583609, steps 1001\n",
      "Episode 0 finished for agent cat with reward -185664.17924491092, -172.2440129583609, steps 1001\n",
      "Episode 0 finished for agent toy with reward 165403.45557829307, 223.8258251408894, steps 1001\n",
      "Episode 0 finished for agent cat with reward -165387.82274808147, -223.8258251408894, steps 1001\n",
      "Episode 0 finished for agent toy with reward 119879.20110900005, 116.24543001770004, steps 1001\n",
      "Episode 0 finished for agent cat with reward -119983.50185532107, -116.24543001770004, steps 1001\n",
      "Episode 0 finished for agent toy with reward 104025.47560935446, 149.0, steps 1001\n",
      "Episode 0 finished for agent cat with reward -104123.05572071197, -149.0, steps 1001\n",
      "Iteration 70: cat: -116678.32, toy: 116596.74 1932.0\n",
      "Episode 0 finished for agent toy with reward 13254.801493583569, 34.48187929913333, steps 237\n",
      "Episode 0 finished for agent cat with reward -13163.007354534087, -34.48187929913333, steps 237\n",
      "Episode 0 finished for agent toy with reward 31889.565886140037, 31.78049716414141, steps 623\n",
      "Episode 0 finished for agent cat with reward -31525.565844318287, -31.78049716414141, steps 623\n",
      "Episode 0 finished for agent toy with reward 103339.44866150008, 147.33974345029924, steps 1001\n",
      "Episode 0 finished for agent cat with reward -103299.72261687674, -147.33974345029924, steps 1001\n",
      "Episode 0 finished for agent toy with reward 91691.12470770717, 61.00819617067857, steps 1001\n",
      "Episode 0 finished for agent cat with reward -91596.0407285138, -61.00819617067857, steps 1001\n",
      "Episode 0 finished for agent toy with reward 129064.35107638825, 91.0, steps 1001\n",
      "Episode 0 finished for agent cat with reward -129120.97973514136, -91.0, steps 1001\n",
      "Episode 0 finished for agent toy with reward 65154.17238381632, 80.0812087820857, steps 1001\n",
      "Episode 0 finished for agent cat with reward -65050.37940210941, -80.0812087820857, steps 1001\n",
      "Episode 0 finished for agent cat with reward -32183.11439732498, -30.0, steps 818\n",
      "Episode 0 finished for agent toy with reward 32023.52414628636, 30.0, steps 818\n",
      "Episode 0 finished for agent toy with reward 42101.413660093065, 22.090722034374522, steps 567\n",
      "Episode 0 finished for agent cat with reward -41917.00247763495, -22.090722034374522, steps 567\n",
      "Episode 0 finished for agent toy with reward 70552.66466390168, 29.732137494637012, steps 743\n",
      "Episode 0 finished for agent cat with reward -70285.73221273234, -29.732137494637012, steps 743\n",
      "Episode 0 finished for agent toy with reward 43737.45313325554, 41.86884283091664, steps 773\n",
      "Episode 0 finished for agent cat with reward -43487.10336195947, -41.86884283091664, steps 773\n",
      "Iteration 80: cat: -62162.86, toy: 62280.85 1553.0\n",
      "Episode 0 finished for agent toy with reward 0.0, 0.0, steps 1\n",
      "Episode 0 finished for agent cat with reward 100.0, 0.0, steps 1\n",
      "Episode 0 finished for agent toy with reward 164664.98159435025, 157.87653403846943, steps 1001\n",
      "Episode 0 finished for agent cat with reward -164711.0994902024, -157.87653403846943, steps 1001\n",
      "Episode 0 finished for agent toy with reward 99840.91167438285, 61.204574992397426, steps 1001\n",
      "Episode 0 finished for agent cat with reward -99797.04893240434, -61.204574992397426, steps 1001\n",
      "Episode 0 finished for agent toy with reward 45209.149803121116, 22.47220505424423, steps 519\n",
      "Episode 0 finished for agent cat with reward -45102.17003305568, -22.47220505424423, steps 519\n",
      "Episode 0 finished for agent toy with reward 24908.66226847772, 21.37755832643195, steps 745\n",
      "Episode 0 finished for agent cat with reward -24561.644046228026, -21.37755832643195, steps 745\n",
      "Episode 0 finished for agent toy with reward 25320.91309393278, 41.10960958218893, steps 257\n",
      "Episode 0 finished for agent cat with reward -25247.466815373253, -41.10960958218893, steps 257\n",
      "Episode 0 finished for agent toy with reward 6298.345913252724, 25.553864678361276, steps 125\n",
      "Episode 0 finished for agent cat with reward -6190.718389195764, -25.553864678361276, steps 125\n",
      "Episode 0 finished for agent toy with reward 62668.16078208986, 20.024984394500787, steps 619\n",
      "Episode 0 finished for agent cat with reward -62314.68165822024, -20.024984394500787, steps 619\n",
      "Episode 0 finished for agent toy with reward 43253.33735508908, 52.20153254455275, steps 1001\n",
      "Episode 0 finished for agent cat with reward -43202.59467736657, -52.20153254455275, steps 1001\n",
      "Episode 0 finished for agent cat with reward -34517.283645823336, -30.805843601498726, steps 344\n",
      "Episode 0 finished for agent toy with reward 34440.62755203199, 30.805843601498726, steps 344\n",
      "Iteration 90: cat: -50554.47, toy: 50660.51 1122.6\n",
      "Episode 0 finished for agent toy with reward 106387.66761752828, 32.01562118716424, steps 771\n",
      "Episode 0 finished for agent cat with reward -106340.17948774854, -32.01562118716424, steps 771\n",
      "Episode 0 finished for agent toy with reward 18756.64562784054, 31.144823004794873, steps 239\n",
      "Episode 0 finished for agent cat with reward -18689.981306727404, -31.144823004794873, steps 239\n",
      "Episode 0 finished for agent toy with reward 25195.96552499303, 20.0, steps 351\n",
      "Episode 0 finished for agent cat with reward -24997.829829350736, -20.0, steps 351\n",
      "Episode 0 finished for agent toy with reward 5674.222822131101, 32.28002478313795, steps 131\n",
      "Episode 0 finished for agent cat with reward -5535.631097113363, -32.28002478313795, steps 131\n",
      "Episode 0 finished for agent toy with reward 33396.143915625245, 35.34119409414458, steps 321\n",
      "Episode 0 finished for agent cat with reward -33185.858744247416, -35.34119409414458, steps 321\n",
      "Episode 0 finished for agent toy with reward 69471.20995024315, 27.65863337187866, steps 649\n",
      "Episode 0 finished for agent cat with reward -69134.22698885287, -27.65863337187866, steps 649\n",
      "Episode 0 finished for agent toy with reward 33319.32225675097, 23.08679276123039, steps 337\n",
      "Episode 0 finished for agent cat with reward -33197.41706291611, -23.08679276123039, steps 337\n",
      "Episode 0 finished for agent cat with reward -42499.32185158741, -30.066592756745816, steps 402\n",
      "Episode 0 finished for agent toy with reward 42335.52061636887, 30.066592756745816, steps 402\n",
      "Episode 0 finished for agent toy with reward 23735.920304589024, 37.8021163428716, steps 351\n",
      "Episode 0 finished for agent cat with reward -23492.7476211443, -37.8021163428716, steps 351\n",
      "Episode 0 finished for agent toy with reward 142829.57434469523, 22.847319317591726, steps 829\n",
      "Episode 0 finished for agent cat with reward -142527.5426316667, -22.847319317591726, steps 829\n",
      "Iteration 100: cat: -49960.07, toy: 50110.22 876.2\n",
      "Episode 0 finished for agent toy with reward 67247.69562595026, 20.8806130178211, steps 573\n",
      "Episode 0 finished for agent cat with reward -66949.27724754803, -20.8806130178211, steps 573\n",
      "Episode 0 finished for agent toy with reward 31186.740106090816, 36.359317925395686, steps 313\n",
      "Episode 0 finished for agent cat with reward -31029.651510794756, -36.359317925395686, steps 313\n",
      "Episode 0 finished for agent toy with reward 93415.6021658325, 22.360679774997898, steps 593\n",
      "Episode 0 finished for agent cat with reward -93276.67464080466, -22.360679774997898, steps 593\n",
      "Episode 0 finished for agent cat with reward -9848.036009425652, -20.0, steps 174\n",
      "Episode 0 finished for agent toy with reward 9707.27588668972, 20.0, steps 174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 学習\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dqn(agent_dict, env_learning, num_iterations, num_episodes_per_iteration)\n",
      "Cell \u001b[0;32mIn[28], line 30\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(agent_dict, env, num_iterations, num_episodes_per_iteration)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m%\u001b[39m replay_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m replay_agent \u001b[38;5;129;01min\u001b[39;00m agent_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 30\u001b[0m             agent_dict[replay_agent]\u001b[38;5;241m.\u001b[39mreplay()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     33\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# No action needed if agent is done\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 82\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(current_q_values\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), target_q_values)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 82\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# εを減少させる\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    650\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[1;32m    354\u001b[0m     tensors,\n\u001b[1;32m    355\u001b[0m     grad_tensors_,\n\u001b[1;32m    356\u001b[0m     retain_graph,\n\u001b[1;32m    357\u001b[0m     create_graph,\n\u001b[1;32m    358\u001b[0m     inputs,\n\u001b[1;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "train_dqn(agent_dict, env_learning, num_iterations, num_episodes_per_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . C . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . T . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "------------------------------------------------------------\n",
      "agent: cat, count: 130, cat: 104, 269, toy: 93, 250\n",
      "mean_reward: (-323782.79339811375, 0.0) +/- (321860.9502775147, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# 評価用環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_eval = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデル評価\n",
    "mean_std_rewards = evaluate_model(agent_dict, env_eval, n_eval_episodes=1)\n",
    "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "save_dqn(agent_dict, \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . C . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". T . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "------------------------------------------------------------\n",
      "agent: toy, count: 957, cat: 426, 513, toy: 38, 151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m load_dqn(env_eval, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoy\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ロードしたモデルの評価\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m mean_std_rewards \u001b[38;5;241m=\u001b[39m evaluate_model(loaded_model, env_eval, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_std_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_std_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 74\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(agent_dict, eval_env, n_eval_episodes)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m         action \u001b[38;5;241m=\u001b[39m agent_dict[agent]\u001b[38;5;241m.\u001b[39mact(obs)  \u001b[38;5;66;03m# 各エージェントに行動させる\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     75\u001b[0m     episode_rewards[agent] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# 各agentごとに報酬を記録\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_name \u001b[38;5;129;01min\u001b[39;00m reward_sums:\n",
      "File \u001b[0;32m~/workspace/cat-brain/cat-dqn/cat_toy_env.py:119\u001b[0m, in \u001b[0;36mCatToyEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_selector\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/workspace/cat-brain/cat-dqn/cat_toy_env.py:152\u001b[0m, in \u001b[0;36mCatToyEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_x\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_y\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, toy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoy_x\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoy_y\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# フレーム間の遅延（1フレームごとの更新時間）\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.05\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 評価用環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_eval = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "loaded_model = load_dqn(env_eval, [\"cat\", \"toy\"], \"models\")\n",
    "\n",
    "# ロードしたモデルの評価\n",
    "mean_std_rewards = evaluate_model(loaded_model, env_eval, n_eval_episodes=10)\n",
    "print(f\"mean_reward: {mean_std_rewards['cat']} +/- {mean_std_rewards['toy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力の2つのTensorを結合\n",
    "toy = torch.randn(1, 2)\n",
    "cat = torch.randn(1, 2)\n",
    "concat_input = torch.cat([toy, cat], dim=1)  # shape: (1, 4)\n",
    "\n",
    "# エクスポート対象モデル（例: policyネットワーク）\n",
    "# dummyの環境\n",
    "env_kwargs=dict(render_mode=\"human\", max_steps=1000)\n",
    "env_dummy = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "loaded_model = load_dqn(env_dummy, [\"cat\", \"toy\"], \"models\")\n",
    "policy_net = loaded_model[\"cat\"].model  # catエージェントのポリシーネットワークを取得\n",
    "\n",
    "# ONNXエクスポート\n",
    "torch.onnx.export(\n",
    "    policy_net,\n",
    "    concat_input,  # ← dictではなく単一Tensor\n",
    "    \"cat_dqn_policy.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    input_names=[\"obs\"],\n",
    "    output_names=[\"q_values\"],\n",
    "    dynamic_axes={\n",
    "        \"obs\": {0: \"batch_size\"},\n",
    "        \"q_values\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境のクローズ\n",
    "env_learning.close()\n",
    "env_eval.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習済みモデルの使用 (play.py)\n",
    "\n",
    "このセルでは、学習済みのモデルをロードし、`CartPole-v1`環境でエージェントがどのように行動するかを観察します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "import time\n",
    "from cat_toy_env import CatToyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs=dict(render_mode=\"\", max_steps=1000, cat_speed = 2)\n",
    "\n",
    "# 環境の作成\n",
    "env = CatToyEnv(**env_kwargs)\n",
    "\n",
    "# モデルのロード\n",
    "model_playing = DQN.load(\"cat_dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エピソードの実行\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model_playing.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(\"観測:\", obs)\n",
    "    done = terminated or truncated\n",
    "    env.render()  # 環境の描画\n",
    "    #time.sleep(0.001) # 0.01秒待機"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境のクローズ\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
